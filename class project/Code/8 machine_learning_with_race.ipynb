{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Template\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This adopts class notebook `machine_learning.ipynb` as a basic template for running machine learning models of interest. Machine learning models are from the `scikit-learn` package. There is also light data wrangling in `pandas`.\n",
    "\n",
    "**To adjust the features of interest first go to \"Data Exploration and Preparation > Feature Generation\" and then \"Creating Training and Validation Sets > Split into Features and Labels.\"** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "---\n",
    "The initial packages including [`scikit-learn`](http://scikit-learn.org) to fit modeling. Note the original tutorial uses `psycopg2` to connect to the database, but we instead use `sqlalchemy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 300)\n",
    "\n",
    "#import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "import matplotlib\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_recall_curve, auc,accuracy_score, precision_score, recall_score,cohen_kappa_score,confusion_matrix\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, \n",
    "AdaBoostClassifier,BaggingClassifier)\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are some basic values used throughout the notebook. Objects `db_name`, `host_name`, and `schema` are database address variables. Objects `train_date`, `test_date`, and `train_horizon` are in a way holdovers from the original tutorial, which uses a very particular form of cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_date = 2010\n",
    "test_date = 2013\n",
    "#prediction_horizon = 3 #orginally 5\n",
    "train_horizon = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_name = \"appliedda\"\n",
    "hostname = \"10.10.2.10\"\n",
    "schema = 'M3'\n",
    "pgsql_engine = create_engine( \"postgresql://10.10.2.10/appliedda\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_string = \" SELECT *\"\n",
    "sql_string +=\" FROM M3.cleaned_data\"\n",
    "\n",
    "full_data = pd.read_sql(sql_string, con = pgsql_engine)    \n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Generation\n",
    "\n",
    "\n",
    "Our preliminary features are the following\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we want our features to be? Let's make one list containing the variables from which we will derive our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DrugVars=['drugalcf','drugampf','drugcocf','drugherf','drugmarf','drugothf','drugpcpf','drugunkf']\n",
    "good_time_vars=['meritorious_good_time','education_in_prison','substanceabuse_treatment','working_in_prison']\n",
    "descriptive_vars=['race', 'sex','HasKids','birthdecade','birthdecade1950orprior','active_gang_member','anypriorwage']\n",
    "prison_vars=['release_year', 'hclass','sexoff','sexreg','lstsclvl','prisontime']\n",
    "\n",
    "feat_source=DrugVars+good_time_vars+descriptive_vars+prison_vars\n",
    "feat_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another issue is that some features will not have the correct data type neccesary for executing the machine learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = full_data.filter(items = feat_source).dtypes\n",
    "print(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, variables of type `object` correspond to strings, the names of which are now collected in `feat_obj`. The `get_dummies()` method treats these variables as categorical and makes a dummy for each. **Run the following cell one time only.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_obj = list(dt[dt == \"object\"].index) #what columns are of type object?\n",
    "temp = pd.get_dummies(full_data.filter(items = feat_obj))\n",
    "feat_get_dummies = list(temp)\n",
    "full_data = full_data.merge(right = temp, how = 'left', left_index = True, right_index = True)\n",
    "full_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_model = list(set(feat_source) - set(feat_obj)) + feat_get_dummies\n",
    "print(feat_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into features and labels\n",
    "Here we can decide which features/predictors to use in our model and what variable to use as the label. The object `feat_model` more or less has this specified for us, but by adjusting `feat_ref` we can remove some variables to serve as reference points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_ref = ['race_WHI', 'sex_F','hclass_M','sexoff_N','sexreg_N','vetf_N',\n",
    "           'drugalcf_N','drugampf_N','drugcocf_N','drugherf_N','drugmarf_N','drugothf_N','drugpcpf_N','drugunkf_N',\n",
    "            'lstsclvl_P','HasKids_N','birthdecade_1920.0','release_year_2010','prisontime_0.0'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_vars = [e for e in feat_model if e.startswith('race_')]\n",
    "print(race_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feat_model = list(set(feat_model) - set(race_vars))\n",
    "sel_features = list(set(feat_model) - set(feat_ref))\n",
    "print(sel_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sel_label= 'employed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training and Validation Data Sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to make training and validation sets. Here are the ranges of years for testing and training currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_years = range(train_date, train_date+train_horizon) \n",
    "test_years = range(test_date, test_date+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The test years include: {yrs}\".format(yrs = test_years))\n",
    "print(\"The training years include: {yrs}\".format(yrs = train_years))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function simply allows for a subset on `exityr`, the year a prisoner was released. The following cells make `train_data` and `test_data` based on the year of ranges above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_test_or_train(yrs):\n",
    "    return(full_data.query(\"exityr in {x}\".format(x=yrs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = create_test_or_train(train_years)\n",
    "test_data = create_test_or_train(test_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the training and testing data\n",
    "\n",
    "The following does some summary statistic style comparisons between `train_data` and `test_data`. This might be a good place to do some visualiztion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What proprotion of individuals in the training set were ever employed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows: {}'.format(train_data.shape[0]))\n",
    "train_data[sel_label].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does that compare to the test data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows: {}'.format(test_data.shape[0]))\n",
    "test_data[sel_label].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that employment happens for more individuals in the training data set. This is not too surprising because the labor market, especially for Illinois, was in bad shape in 2010 and 2011 compared to 2013.\n",
    "\n",
    "The following grabs the mean and standard error for the `race_` dummies to see how the proportion of different races varies, if at all, across the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "race_mean_train = train_data.filter(regex = \"^race_\").mean()\n",
    "race_se_train = train_data.filter(regex = \"^race_\").std()/np.sqrt(train_data.shape[0])\n",
    "race_mean_test = test_data.filter(regex = \"^race_\").mean()\n",
    "race_se_test = train_data.filter(regex = \"^race_\").std()/np.sqrt(test_data.shape[0])\n",
    "race_summ = pd.DataFrame({'train_mean' : race_mean_train,\n",
    "                          'train_se' : race_se_train,\n",
    "                          'test_mean' : race_mean_test,\n",
    "                          'test_se' : race_se_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(race_summ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the proportion of Black releasees may be slightly higher and the proportion of White and Hispanic releasees may be slightly lower in the training model compared to the test model. \n",
    "\n",
    "The last cell look at birth year. The distribution of ages appears to be roughly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_birth_year = pd.DataFrame({'train' : train_data['birth_year'].describe(),\n",
    "                                'test' : test_data['birth_year'].describe()})\n",
    "print(summ_birth_year)                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This splits the train and test data into features and labels. They are float arrays, so basically matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use conventions typically used in python scikitlearn\n",
    "X_train = train_data[sel_features].values\n",
    "y_train = train_data[sel_label].values\n",
    "X_test = test_data[sel_features].values\n",
    "y_test = test_data[sel_label].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very little is edited here compared to the orginal.\n",
    "\n",
    "## Model Evaluation \n",
    "\n",
    "\n",
    "In this phase, you take the predictors from your test set and apply your model to them, then assess the quality of the model by comparing the *predicted values* to the *actual values* for each record in your testing data set. \n",
    "\n",
    "- **Performance Estimation**: How well will our model do once it is deployed and applied to new data?\n",
    "\n",
    "Now let's use the model we just fit to make predictions on our test dataset, and see what our accuracy score is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation \n",
    "\n",
    "Machine learning models usually do not produce a prediction (0 or 1) directly. Rather, models produce a score (that can sometimes be interpreted a a probabilty) between 0 and 1, which lets you more finely rank all of the examples from *most likely* to *least likely* to have label 1 (positive). This score is then turned into a 0 or 1 based on a user-specified threshold. For example, you might label all examples that have a score greater than 0.5 (1/2) as positive (1), but there's no reason that has to be the cutoff. \n",
    "\n",
    "This function makes plots for the normal train-test approach and cross-validation. Couldn't think of a better place to put it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes the data frame generated in `cv_year_metrics()` and creates a bar chart summarizing a chosen model metric for each year for all models, or all model metrics averaged across years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_year_metrics_plot(df, chosen_metric = 'accuracy', time_agg = False, aspect=1):\n",
    "    if time_agg == False: #for each model plot the given metric by year\n",
    "        pp = sns.factorplot(x = 'year', y = 'value', hue = 'model', \\\n",
    "                            data = df[df['metric'] == chosen_metric], \\\n",
    "                            kind='bar', aspect = aspect)\n",
    "    elif time_agg == True: #for each model plot all metrics avged over year\n",
    "        dfx = df.groupby(['metric', 'model'], as_index = False)['value'].mean()\n",
    "        pp = sns.factorplot(x = 'metric', y = 'value', hue = 'model', \\\n",
    "                            data = dfx, kind = 'bar', aspect = aspect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "Once we have tuned our scores to 0 or 1 for classification, we create a *confusion matrix*, which  has four cells: true negatives, true positives, false negatives, and false positives. Each data point belongs in one of these cells, because it has both a ground truth and a predicted label. If an example was predicted to be negative and is negative, it's a true negative. If an example was predicted to be positive and is positive, it's a true positive. If an example was predicted to be negative and is positive, it's a false negative. If an example was predicted to be positive and is negative, it's a false negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count of true negatives is `conf_matrix[0,0]`, false negatives `conf_matrix[1,0]`, true positives `conf_matrix[1,1]`, and false_positives `conf_matrix[0,1]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we care about our whole precision-recall space, we can optimize for a metric known as the **area under the curve (AUC-PR)**, which is the area under the precision-recall curve. The maximum AUC-PR is 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall(y_true,y_score):\n",
    "    \"\"\"\n",
    "    Plot a precision recall curve\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: ls\n",
    "        ground truth labels\n",
    "    y_score: ls\n",
    "        score output from model\n",
    "    \"\"\"\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true,y_score)\n",
    "    plt.plot(recall_curprecision_recall_curveve, precision_curve)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    auc_val = auc(recall_curve,precision_curve)\n",
    "    print('AUC-PR: {0:1f}'.format(auc_val))\n",
    "    plt.show()\n",
    "    #plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall at k%\n",
    "\n",
    "If we only care about a specific part of the precision-recall curve we can focus on more fine-grained metrics. For instance, say there is a special program for people likely to be recidivists, but only 5% can be admitted. In that case, we would want to prioritize the 5% who were *most likely* to end up back in jail, and it wouldn't matter too much how accurate we were on the 80% or so who weren't very likely to end up back in jail. \n",
    "\n",
    "Let's say that, out of the approximately 200,000 prisoners, we can intervene on 5% of them, or the \"top\" 10,000 prisoners (where \"top\" means highest predicted risk of recidivism). We can then focus on optimizing our **precision at 5%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall_n(y_true, y_prob, model_name):\n",
    "    \"\"\"\n",
    "    y_true: ls\n",
    "        ls of ground truth labels\n",
    "    y_prob: ls\n",
    "        ls of predic proba from model\n",
    "    model_name: str\n",
    "        str of model name (e.g, LR_123)\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    y_score = y_prob\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
    "    precision_curve = precision_curve[:-1]\n",
    "    recall_curve = recall_curve[:-1]\n",
    "    pct_above_per_thresh = []\n",
    "    number_scored = len(y_score)\n",
    "    for value in pr_thresholds:\n",
    "        num_above_thresh = len(y_score[y_score>=value])\n",
    "        pct_above_thresh = num_above_thresh / float(number_scored)\n",
    "        pct_above_per_thresh.append(pct_above_thresh)\n",
    "    pct_above_per_thresh = np.array(pct_above_per_thresh)\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(pct_above_per_thresh, precision_curve, 'b')\n",
    "    ax1.set_xlabel('percent of population')\n",
    "    ax1.set_ylabel('precision', color='b')\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(pct_above_per_thresh, recall_curve, 'r')\n",
    "    ax2.set_ylabel('recall', color='r')\n",
    "    ax2.set_ylim(0,1.05)\n",
    "    \n",
    "    name = model_name\n",
    "    plt.title(name)\n",
    "    #plt.show()\n",
    "    #plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, y_scores,k):\n",
    "    \n",
    "    threshold = np.sort(y_scores)[::-1][int(k*len(y_scores))]\n",
    "    y_pred = np.asarray([1 if i >= threshold else 0 for i in y_scores ])\n",
    "    return precision_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "calc_threshold = lambda x,y: 0 if x < y else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "One or more functions that perform tasks in the user-defined machine learning functions.\n",
    "\n",
    "The function `expand_grid()` creates a `DataFrame` object with a row for ever possible combination of the dictionary in the argument. It recreates R's `expand.grid()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def expand_grid(data_dict):\n",
    "    rows = itertools.product(*data_dict.values())\n",
    "    return pd.DataFrame.from_records(rows, columns = data_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_params(X, Y, year_group, class_fire, tuning_parameters):\n",
    "    #what are the scores that we care about?\n",
    "    scores = ['roc_auc', 'precision']\n",
    "    \n",
    "    #classifiers of interest\n",
    "    c_f = clfs[class_fire]\n",
    "    \n",
    "    #split the data into our preferred train-test combo\n",
    "    X_train = X[np.in1d(year_group, [2011, 2012, 2013])]\n",
    "    X_test = X[year_group == 2010]\n",
    "    Y_train = Y[np.in1d(year_group, [2011, 2012, 2013])]\n",
    "    Y_test = Y[year_group == 2010]\n",
    "                                                      \n",
    "    #create the cross-validation indices by year\n",
    "    #gkf = GroupKFold(n_splits = 4)\n",
    "    #gkf.split(X, Y, groups=year_group)\n",
    "    \n",
    "    for score in scores:\n",
    "        print(\"Tuning hyperparamters for %s\" % score)\n",
    "        print()\n",
    "        \n",
    "        cxx = GridSearchCV(c_f, tuning_parameters, cv = 5, \\\n",
    "                          scoring = '%s' % score)\n",
    "        cxx.fit(X_train, Y_train)\n",
    "        \n",
    "        print(\"Best parameters set foundon development set:\")\n",
    "        print()\n",
    "        print(cxx.best_params_)\n",
    "        print()\n",
    "        means = cxx.cv_results_['mean_test_score']\n",
    "        stds = cxx.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, cxx.cv_results_['params']):\n",
    "            print(\"%f (+/- %f) for %r\" % (mean, 2*std, params))\n",
    "        print()\n",
    "        \n",
    "        print(\"Detailed Classification Report:\")\n",
    "        print(\"The model is trained on the full development set.\")\n",
    "        print(\"The scores are evaluated on the full evaluation set.\")\n",
    "        print()\n",
    "        Y_true, Y_pred = Y_test, cxx.predict(X_test)\n",
    "        print(classification_report(Y_true, Y_pred))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns cross-validated (grouped by `exit_yr`) model assessment scores (e.g., accuracy, recall, etc.) for every classifier listed in `sel_clfs`. All data is stored in a pandas data frame. \n",
    "\n",
    "Adjust which score measures you want by modifying the list `metrics` within the below function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_year_metrics(X, Y, year_group, class_fires):\n",
    "    #create objects to loop over, store results\n",
    "    yrs = range(2010, 2014)\n",
    "    metrics = ['accuracy', 'recall', 'precision', 'roc_auc'] #can add more\n",
    "    df = expand_grid({'year' : yrs,\n",
    "                     'metric' : metrics,\n",
    "                     'model' : class_fires}) \n",
    "    df['value'] = np.NAN\n",
    "    \n",
    "    #create the cross-validation indices by year\n",
    "    gkf = GroupKFold(n_splits = 4)\n",
    "    gkf.split(X, Y, groups=year_group)\n",
    "    \n",
    "    #compute cross-validation metrics over each model\n",
    "    for fires in class_fires: \n",
    "        for mm in metrics:\n",
    "            temp_fire = clfs[fires]\n",
    "            df_ind = (df['model'] == fires) & (df['metric'] == mm)\n",
    "            df.loc[df_ind, 'value'] = cross_val_score(temp_fire, X, np.ravel(Y), \\\n",
    "                                                      groups=year_group, scoring=mm, cv=gkf)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function fits a logistic regression model for every train-test year combination and returns the estimated model coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_lr_coef(X, Y, year_group, feat_names):\n",
    "    #create cv indices\n",
    "    gkf = GroupKFold(n_splits = 4)\n",
    "    cv_inds = list(gkf.split(X, Y, groups=year_group))\n",
    "    \n",
    "    #initialize data set\n",
    "    df = pd.DataFrame({'feat' : feat_names})\n",
    "    \n",
    "    #obtain coefs for each training fold\n",
    "    for x in range(0,4):\n",
    "        lm = clfs['LogisticReg']\n",
    "        fitx = lm.fit(X.loc[cv_inds[x][0]], Y.loc[cv_inds[x][0]])\n",
    "        df['yr201{xx}'.format(xx = x)] = fitx.coef_[0]\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `cv_lr_coef`, this function returns the feature importance rankings obtained from a random forest classifier by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_rf_feat_import(X, Y, year_group, feat_names):\n",
    "    #create cv indices\n",
    "    gkf = GroupKFold(n_splits = 4)\n",
    "    cv_inds = list(gkf.split(X, Y, groups=year_group))\n",
    "    \n",
    "    #initialize data set\n",
    "    df = pd.DataFrame({'feat' : feat_names})\n",
    "    \n",
    "    #obtain coefs for each training fold\n",
    "    for x in range(0,4):\n",
    "        rf = clfs['RandomForest']\n",
    "        rf.fit(X.loc[cv_inds[x][0]], Y.loc[cv_inds[x][0]])\n",
    "        importances = rf.feature_importances_\n",
    "        df['yr201{xx}'.format(xx = x)] = importances\n",
    "        #temp = fitx.coef_[0]\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Pipeline\n",
    "*[Go back to Table of Contents](#table-of-contents)*\n",
    "\n",
    "When working on machine learning projects, it is a good idea to structure your code as a modular **pipeline**, which contains all of the steps of your analysis, from the original data source to the results that you report, along with documentation. This has many advantages:\n",
    "- **Reproducibility**. It's important that your work be reproducible. This means that someone else should be able\n",
    "to see what you did, follow the exact same process, and come up with the exact same results. It also means that\n",
    "someone else can follow the steps you took and see what decisions you made, whether that person is a collaborator, \n",
    "a reviewer for a journal, or the agency you are working with. \n",
    "- **Ease of model evaluation and comparison**.\n",
    "- **Ability to make changes.** If you receive new data and want to go through the process again, or if there are \n",
    "updates to the data you used, you can easily substitute new data and reproduce the process without starting from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfs = {'RandomForest': RandomForestClassifier(n_estimators=500, n_jobs=-1),\n",
    "        'ExtraTrees': ExtraTreesClassifier(n_estimators=10, n_jobs=-1, criterion='entropy'),\n",
    "        'LogisticReg': LogisticRegression(penalty='l1', C=1e5),\n",
    "        'StochasticGradientDescent':SGDClassifier(loss='log'),\n",
    "        'GradientBoosting': GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=10),\n",
    "        'NaiveBayes': GaussianNB(),\n",
    "        'SupportVectorMachine': SVC(probability=True),\n",
    "        'LinearSupportVectorMachine': LinearSVC(),\n",
    "        'NearestNeighbor': KNeighborsClassifier(),\n",
    "        'BaggedNearestNeighbor':BaggingClassifier(KNeighborsClassifier(),max_samples=0.5,max_features=0.5),\n",
    "        'MostFreqDummy':DummyClassifier(strategy='most_frequent'),\n",
    "        'StratifiedDummy':DummyClassifier(strategy='stratified'),\n",
    "        'UniformDummy':DummyClassifier(strategy='uniform')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sel_clfs = ['RandomForest', 'ExtraTrees', 'LogisticReg', 'StochasticGradientDescent', 'GradientBoosting', 'NaiveBayes','NearestNeighbor','BaggedNearestNeighbor','MostFreqDummy','StratifiedDummy','UniformDummy']\n",
    "#sel_clfs = ['RandomForest']#,'LogisticReg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print(sel_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name_set=[]\n",
    "accuracy_set = []\n",
    "precision_set = []\n",
    "recall_set = []\n",
    "kappa_set = []\n",
    "p_at_1_set=[]\n",
    "p_at_5_set=[]\n",
    "max_p_at_k = 0\n",
    "for clfNM in sel_clfs:\n",
    "    model_name_set.append(clfNM)\n",
    "    clf = clfs[clfNM]\n",
    "    clf.fit( X_train, y_train )\n",
    "    print clf\n",
    "    if clfNM=='LogisticReg':\n",
    "        print \"The coefficients and standard deviations for each of the features are \" \n",
    "        std_coef = np.std(X_test,0)*clf.coef_\n",
    "        zip(sel_features, clf.coef_[0].round(3),std_coef[0].round(3))\n",
    "    y_score = clf.predict_proba(X_test)[:,1]\n",
    "    hist, bin_edges = np.histogram(y_score, bins='fd')\n",
    "    safe_ind = np.where(hist > 10)[0] #returns indices where count > 10\n",
    "    safe_bins = bin_edges[safe_ind]\n",
    "    print(\"Here are the bin counts:\")\n",
    "    print(np.histogram(y_score, bins = safe_bins)[0])\n",
    "    plt.figure()\n",
    "    sns.distplot(y_score, kde=True, rug=False, bins=safe_bins, norm_hist=True)\n",
    "    #save the distribution plot\n",
    "    savefig(\"/nfshome/pmclaughlin/Projects/M3/user/pmclaughlin/project-repository/to export/images/with_race/score_dist_{x}.pdf\".format(x = clfNM), \\\n",
    "           bbox_inches = \"tight\")\n",
    "    #close the plot\n",
    "    plt.clf()\n",
    "    predicted = np.array(y_score)\n",
    "    predictedVal = np.array( [calc_threshold(score,0.5) for score in y_score] )\n",
    "    expected = np.array(y_test)\n",
    "    #evaluation metrics\n",
    "    conf_matrix = confusion_matrix(expected,predictedVal)\n",
    "    accuracy = accuracy_score(expected, predictedVal)\n",
    "    accuracy_set.append(accuracy)\n",
    "    precision = precision_score(expected, predictedVal)\n",
    "    precision_set.append(precision)\n",
    "    recall = recall_score(expected, predictedVal)\n",
    "    recall_set.append(recall)\n",
    "    kappa = cohen_kappa_score(expected, predictedVal)\n",
    "    kappa_set.append(kappa)\n",
    "    print conf_matrix\n",
    "    print( \"Accuracy = \" + str( accuracy))\n",
    "    print( \"Precision = \" + str(precision))\n",
    "    print( \"Recall= \" + str(recall))\n",
    "    print( \"Kappa= \" + str(kappa))\n",
    "    p_at_1 = precision_at_k(expected,y_score, 0.01)\n",
    "    p_at_5 = precision_at_k(expected,y_score, 0.05)\n",
    "    p_at_1_set.append(p_at_1)\n",
    "    p_at_5_set.append(p_at_5)\n",
    "    print('Precision at 1%: {:.2f}'.format(p_at_1))\n",
    "    print('Precision at 5%: {:.2f}'.format(p_at_5))\n",
    "    if max_p_at_k < p_at_5:\n",
    "        max_p_at_k = p_at_5\n",
    "\n",
    "    #plot_precision_recall(expected, y_score)\n",
    "    plt.figure()\n",
    "    plot_precision_recall_n(expected,predicted, clfNM)\n",
    "    savefig(\"/nfshome/pmclaughlin/Projects/M3/user/pmclaughlin/project-repository/to export/images/with_race/roc_plot_{x}.pdf\".format(x = clfNM), \\\n",
    "           bbox_inches = \"tight\")\n",
    "    plt.clf()\n",
    "    \n",
    "print(max_p_at_k)\n",
    "evaluation_metrics_dict={'Model': model_name_set,\n",
    "                         'Accuracy': accuracy_set,\n",
    "                         'Precision': precision_set,\n",
    "                         'Recall': recall_set,\n",
    "                         'Kappa': kappa_set,\n",
    "                         'Precision_at_1_pct': p_at_1_set,\n",
    "                         'Precision_at_5_pct': p_at_5_set}\n",
    "evaluation_metrics=pd.DataFrame.from_dict(evaluation_metrics_dict)\n",
    "evaluation_metrics\n",
    "evaluation_metrics.to_csv(path_or_buf = \"/nfshome/pmclaughlin/Projects/M3/user/pmclaughlin/project-repository/to export/holdout_metrics_with_race.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evm_melt = pd.melt(evaluation_metrics, id_vars = ['Model'], var_name = 'metric')\n",
    "evm_melt = evm_melt.rename(index = str, columns = {'Model': 'model'})\n",
    "cv_year_metrics_plot(evm_melt, time_agg=True, aspect = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This breaks out the feature variables as `X` and stores `exit_year` as a grouping variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = full_data[sel_features]#.select_dtypes(exclude=['object']).values\n",
    "group = full_data['exityr'].values\n",
    "#temp_feat = list(full_data[sel_features].select_dtypes(exclude=['object']))\n",
    "employed = full_data['employed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shape(X))\n",
    "print(shape(group))\n",
    "print(shape(employed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, model evaluation measures are presented individually across years and then averaged across years. Next, coefficients and feature importance for the logistic regression and random forest classifiers, respectively, are presented.\n",
    "\n",
    "## Model Evalation By Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_stats = cv_year_metrics(X, employed, group, sel_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_stats.to_csv(path_or_buf = \"/nfshome/pmclaughlin/Projects/M3/user/pmclaughlin/project-repository/to export/cv_metrics_without_race.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_year_metrics_plot(model_stats, chosen_metric = 'accuracy', aspect=2)\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_year_metrics_plot(model_stats, chosen_metric = 'precision', aspect=2)\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_year_metrics_plot(model_stats, chosen_metric = 'recall', aspect=2)\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_year_metrics_plot(model_stats, chosen_metric = 'roc_auc', aspect=2)\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation Averaged Over Years\n",
    "\n",
    "Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_year_metrics_plot(model_stats, time_agg = True, aspect = 2)\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Logistic Regression by Year\n",
    "\n",
    "**Keep in mind that `employed==1` means unemployed**\n",
    "\n",
    "For the most part, it seems that the sign and magnitude of the LR coefficients are robust to whatever chosen year is the holdout. A few exceptions:\n",
    "\n",
    "* `meritorious_good_time` has a much smaller magnitude for 2012 and 2013 as holdouts compared to 2010 and 2013.\n",
    "* `drugothf_Y`, `hclass_U` has a positive coefficient for 2011 holdout but is negative in all others.\n",
    "* `working_in_prison` seems to have a high-varying magnitude\n",
    "* `drugunkf_Y` has a much higher magnitude for 2010 holdout than all other years\n",
    "* `lstsclvl_1` has a negative magnitue for 2012 holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_assess = cv_lr_coef(X, employed, group, feat_names = sel_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_assess.to_csv(path_or_buf = \"/nfshome/pmclaughlin/Projects/M3/user/pmclaughlin/project-repository/to export/cv_lr_with_race.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr_assess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Feature Importance by Year\n",
    "\n",
    "The basic relative importance of features appears robust across years. The variables that seem to matter the most are `birthdecade`, `prisontime`, and `anypriorwage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_assess = cv_rf_feat_import(X, employed, group, feat_names = sel_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_assess.to_csv(path_or_buf = \"/nfshome/pmclaughlin/Projects/M3/user/pmclaughlin/project-repository/to export/cv_rf_with_race.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_assess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline \n",
    "\n",
    "It is important to check our model against a reasonable **baseline** to know how well our model is doing. Without any context, 83% accuracy can sound really great... but it's not so great when you remember that you could do almost that well by declaring everyone a non-recividist, which would be stupid (not to mention useless) model. \n",
    "\n",
    "A good place to start is checking against a *random* baseline, assigning every example a label (positive or negative) completely at random. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The remaining cells are not run because they do not apply yet**\n",
    "\n",
    "Another good practice is checking against an \"expert\" or rule of thumb baseline. For example, say that talking to people at the DOC, you find that they think it's much more likely that someone who has been in prison multiple times already will reoffend. Then you should check that your classifier does better than just labeling everyone who has had multiple past admits as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#recidivism_predicted = np.array([ 1 if nadmit > 1 else 0 for nadmit in test_data.nadmits.values ])\n",
    "#recidivism_p_at_5 = precision_at_k(expected,recidivism_predicted,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all_non_recidivist = np.array([0 for nadmit in df_testing.nadmits.values])\n",
    "#all_non_recidivist_p_at_5 = precision_at_k(expected, all_non_recidivist,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sns.set_style(\"whitegrid\")\n",
    "#fig, ax = plt.subplots(1, figsize=(22,12))\n",
    "#sns.set_context(\"poster\", font_scale=1.25, rc={\"lines.linewidth\":2.25, \"lines.markersize\":8})\n",
    "#sns.barplot(['Random','All Non-Recidivist', 'Recidivism','Model'],\n",
    "#            [random_p_at_5, all_non_recidivist_p_at_5, recidivism_p_at_5, max_p_at_k],\n",
    "#            palette=['#6F777D','#6F777D','#6F777D','#800000'])\n",
    "#plt.ylim(0,1)\n",
    "#plt.ylabel('precision at 5%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's explore some of the models we just built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore random forest RF\n",
    "sel_clfs\n",
    "clf = clfs['RandomForest']\n",
    "#clf = clfs[clfNM]\n",
    "print clf\n",
    "clf.fit( X_train, y_train )\n",
    "print clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see if we can make this look a little better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_\n",
    "std = np.std ([tree.feature_importances_ for tree in clf.estimators_],\n",
    "       axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print (\"Feature ranking\")\n",
    "for f in range(X_test.shape[1]):\n",
    "    print (\"%d. %s (%f)\" % (f + 1, sel_features[indices[f]], importances[indices[f]]))\n",
    "\n",
    "# plot \n",
    "plt.figure\n",
    "plt.title (\"Feature Importances\")\n",
    "plt.bar(range(X_test.shape[1]), importances[indices], color='r',\n",
    "      yerr=std[indices], align = \"center\")\n",
    "plt.xticks(range(X_test.shape[1]), sel_features, rotation=90)\n",
    "plt.xlim([-1, X_test.shape[1]])\n",
    "plt.show\n",
    "savefig(\"/nfshome/pmclaughlin/Projects/M3/user/pmclaughlin/project-repository/to export/images/with_race/feat_import_plt.pdf\", \\\n",
    "       bbox_inches = \"tight\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise \n",
    "\n",
    "Our model has just scratched the surface. Try the following: \n",
    "    \n",
    "- Create more features\n",
    "- Try more models\n",
    "- Try different parameters for your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "*[Go back to Table of Contents](#table-of-contents)*\n",
    "\n",
    "- Hastie et al.'s [The Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/) is a classic and is available online for free.\n",
    "- James et al.'s [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/), also available online, includes less mathematics and is more approachable.\n",
    "- Wu et al.'s [Top 10 Algorithms in Data Mining](http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
