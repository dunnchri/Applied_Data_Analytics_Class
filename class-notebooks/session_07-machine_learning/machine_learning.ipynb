{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Introduction](#introduction)\n",
    "- [Setup](#setup)\n",
    "- [Problem Formulation](#model-formulation)\n",
    "- [Creating Labels (Outcomes)]\n",
    "- [Feature Generation](#feature-generation)\n",
    "- [Model Fitting](#model-fitting)\n",
    "- [Model Evaluation](#model-evaluation)\n",
    "- [Machine Learning Pipeline](#machine-learning-pipeline)\n",
    "- [Deployment](#deployment)\n",
    "- [Exercises](#exercises)\n",
    "- [Resources](#resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this tutorial, we'll discuss how to formulate a policy problem or a social science question in the machine learning framework; how to transform raw data into something that can be fed into a model; how to build, evaluate, compare, and select models; and how to reasonably and accurately interpret model results. You'll also get hands-on experience using the `scikit-learn` package in Python with data you're familiar with from previous tutorials. \n",
    "\n",
    "\n",
    "This tutorial is based on chapter 6 of [Big Data and Social Science](https://github.com/BigDataSocialScience/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "---\n",
    "*[Back to Table of Contents](#table-of-contents)*\n",
    "\n",
    "Before we begin, run the code cell below to initialize the libraries we'll be using in this assignment. We're already familiar with `numpy`, `pandas`, and `psycopg2` from previous tutorials. Here we'll also be using [`scikit-learn`](http://scikit-learn.org) to fit modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, \n",
    "AdaBoostClassifier)\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sqlalchemy import create_engine\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_name = \"appliedda\"\n",
    "hostname = \"10.10.2.10\"\n",
    "schema = 'class1'\n",
    "train_date = 2006\n",
    "test_date = 2011\n",
    "prediction_horizon = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(database=db_name, host = hostname) #database connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Formulation\n",
    "*[Go back to Table of Contents](#table-of-contents)*\n",
    "\n",
    "  \n",
    "## Our Machine Learning Problem\n",
    ">Of all prisoners released, we would like to predict who is likely to reenter jail within *5* years of the day we make our prediction. For instance, say it is Jan 1, 2012 and we want to identify which \n",
    ">prisoners are likely to re-enter jail between now and end of 2016. We can run our predictive model and identify >who is most likely at risk. The is an example of a *binary classification* problem. \n",
    "\n",
    "Note the outcome window of 5 years is completely arbitrary. You could use a window of 5, 3, 1 years or 1 day. \n",
    "\n",
    "# Data Exploration and Preparation. \n",
    "\n",
    "In order to predict recidivism, we will be using data from the `ildoc_admit` and `ildoc_exit` table to create **labels** and **features**. \n",
    "\n",
    "\n",
    "# Building a Model\n",
    "\n",
    "We need to munge our dataset into **features** (predictors, or dependent variables, or $X$ variables) and **labels** (independent variables, or $Y$ variables).  For ease of reference, in subsequent examples, names of variables that pertain to predictors will start with \"`X_`\", and names of variables that pertain to outcome variables will start with \"`y_`\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Labels (or Outcome Variables)\n",
    "\n",
    "We can write SQL code in `psql`, `dbeaver`, `pgAdmin`, or programmaticaly generate the SQL and pass to the DB using `psycopg2` to create the labels. Here is the SQL code for developing labels: \n",
    "```\n",
    "-- creating labels for the training set. Of all the people (docnbr) that exited before the\n",
    "-- end of 2005 get the discharge date for all prisoners release dates \n",
    "\n",
    "--release dates table grabs the docnbr and release year for all exists in the data\n",
    "drop table if exists release_dates;\n",
    "create table release_dates as\n",
    "select docnbr, actdisyr, actmsryr,\n",
    "case when actdisyr = 0 or actdisyr is null then actmsryr else actdisyr end release_yr\n",
    "from ildoc_exit;\n",
    "\n",
    "--grab the docnbr and release years for everyone from the years 1989-2005\n",
    "--using are previous release_dates table\n",
    "drop table if exists release_dates_1989_2005;\n",
    "create temp table release_dates_1989_2005 as\n",
    "select docnbr, release_yr\n",
    "from release_dates\n",
    "where release_yr between 1989 and 2005;\n",
    "\n",
    "--there are cases where people have entered and existed prison\n",
    "--multiple times, in those cases grab the last release year in \n",
    "--our time range. \n",
    "drop table if exists last_exit_1989_2005;\n",
    "create temp table last_exit_1989_2005 as\n",
    "select docnbr, max(release_yr) last_release_yr\n",
    "from release_dates_1989_2005\n",
    "group by docnbr;\n",
    "\n",
    "--find the peple admitted from 2006 to 2010. \n",
    "drop table if exists admit_2006_2010;\n",
    "create temp table admit_2006_2010 as\n",
    "select docnbr, curadmyr\n",
    "from ildoc_admit\n",
    "where curadmyr between 2006 and 2010;\n",
    "\n",
    "--join the exit and admit data where 0 is for no readmit\n",
    "-- and 1 for readmit\n",
    "drop table if exists recidivism_2005_2010;\n",
    "create temp table recidivism_2005_2010 as\n",
    "select r.docnbr, r.last_release_yr, a.curadmyr,\n",
    "case when a.curadmyr is null then 0 else 1 end recidivism\n",
    "from last_exit_1989_2005 r\n",
    "left join admit_2006_2010 a on r.docnbr = a.docnbr;\n",
    "\n",
    "-- create the labels table\n",
    "drop table if exists recidivism_labels_2005_2010;\n",
    "create table recidivism_labels_2005_2010 as\n",
    "select distinct docnbr, recidivism\n",
    "from recidivism_2005_2010;\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this SQL script, we first created the table `release_dates` where we took the docnbr (unique identifier) and created a new field `release_yr` which takes on the value of `actmsryr` if `actdisyr` is null or zero. Since prisoners are either released through supervised work release, `actmsryr`, or discharged directly from prison, `actdisyr`, we take the value of  `actdisyr` if it exists, or `actmsryr` if `actdisyr` doesn't exist. \n",
    "\n",
    "Next we created a table `release_dates_1989_2005`, which used the `release_dates` table we just created. We took all of the records for `docnbr` and `release_yr` between 1989 and 2005. Next we created a table `last_exit_1989_2005`, which takes the *maximum* (most recent) `release_yr` for every `docnbr` and writes into `last_exit_1989_2005`. This table will only have one entry per `doc_nbr`, so for any given `docnbr`, or individual, we know their *most recent* release year.\n",
    "\n",
    "We then find everyone admitted into prison between 2006 and 2010, and do a `left join`  on the `last_exit_1989_2005` (left) table and the `recidivism_2005_2010` (right) table on the `docnbr` field. The resulting table will keep all the entries from the *left* table (most recent releases between 1989 and 2005) and add their admits between 2006 and 2010. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(prediction_date, prediction_horizon, conn, schema='class1', overwrite=False):\n",
    "    \"\"\"\n",
    "    Generate a list of labels and return the \n",
    "    table as a dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction_date: int\n",
    "        for simplicity, we will use just the year and assume 2006 means 1/1/2006\n",
    "    prediction_horizon: int\n",
    "        number of years within which we want to predict if they re-enter prison.\n",
    "        for example, for prediction_date = 2006 and prediction_horizon = 5, we will get everyone\n",
    "        who re-entered from 1/1/2006 to 12/31/2010\n",
    "    overwrite: bool\n",
    "        if True runs the query if table does\n",
    "        not exist\n",
    "    conn: obj\n",
    "        psycopg2 conection object to database\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    df_labels: DataFrame\n",
    "        Dataframe of labels\n",
    "    \"\"\"\n",
    "    begin_range = prediction_date\n",
    "    end_range = prediction_date + prediction_horizon\n",
    "    \n",
    "    # check if the table you're trying to create already exists\n",
    "    cursor = conn.cursor()\n",
    "    query = \"\"\"\n",
    "            select * from information_schema.tables \n",
    "            where table_name=\\'recidivism_labels_{begin_range}_{end_range}\\'\n",
    "            \"\"\".format(begin_range=begin_range,\n",
    "                      end_range=end_range);\n",
    "    cursor.execute(query) \n",
    "    \n",
    "    if not(cursor.rowcount) or overwrite:\n",
    "        print('generating labels')\n",
    "        sql_script=\"\"\"\n",
    "    -- creating labels for the training set. Of all the people (docnbr) that exited before \n",
    "    --  {begin_range}\n",
    "\n",
    "    drop table if exists {schema}.recidivism_labels_{begin_range}_{end_range}; \n",
    "\n",
    "    -- get the discharge date for all prisoners release dates \n",
    "    drop table if exists {schema}.release_dates;\n",
    "    create table {schema}.release_dates as\n",
    "    select docnbr, actdisyr, actmsryr,\n",
    "    case when actdisyr = 0 or actdisyr is null then actmsryr else actdisyr end release_yr\n",
    "    from ildoc.ildoc_exit;\n",
    "\n",
    "    commit;\n",
    "\n",
    "    --grab the docnbur and release years for everyone from the years 1989 to the day before our prediction date\n",
    "    --generate labels\n",
    "    drop table if exists release_dates_1989_{begin_range};\n",
    "    create temp table release_dates_1989_{begin_range} as\n",
    "    select docnbr, release_yr\n",
    "    from {schema}.release_dates\n",
    "    where release_yr >= 1989 and release_yr < {begin_range};\n",
    "\n",
    "    commit;\n",
    "\n",
    "    -- find the last time they exited from the previous table\n",
    "    drop table if exists last_exit_1989_{begin_range};\n",
    "    create temp table last_exit_1989_{begin_range} as\n",
    "    select docnbr, max(release_yr) last_release_yr\n",
    "    from release_dates_1989_{begin_range}\n",
    "    group by docnbr;\n",
    "\n",
    "    commit;\n",
    "\n",
    "    -- generate a table that gives us all admits that happen between our start and end dates for labels\n",
    "    drop table if exists admit_{begin_range}_{end_range};\n",
    "    create temp table admit_{begin_range}_{end_range} as\n",
    "    select docnbr, curadmyr\n",
    "    from ildoc.ildoc_admit\n",
    "    where curadmyr >= {begin_range} and  curadmyr < {end_range};\n",
    "\n",
    "    commit;\n",
    "\n",
    "    -- for all people released between 1989 and the date of prediction, check to see if they were admitted again \n",
    "    -- during our begin and end dates\n",
    "    drop table if exists recidivism_{begin_range}_{end_range};\n",
    "    create temp table recidivism_{begin_range}_{end_range} as\n",
    "    select r.docnbr, r.last_release_yr, a.curadmyr,\n",
    "    case when a.curadmyr is null then 0 else 1 end recidivism\n",
    "    from last_exit_1989_{begin_range} r\n",
    "    left join admit_{begin_range}_{end_range} a on r.docnbr = a.docnbr;\n",
    "\n",
    "    commit;\n",
    "\n",
    "    -- drop duplicates\n",
    "    drop table if exists {schema}.recidivism_labels_{begin_range}_{end_range};\n",
    "    create table {schema}.recidivism_labels_{begin_range}_{end_range} as\n",
    "    select distinct docnbr, recidivism\n",
    "    from recidivism_{begin_range}_{end_range};\n",
    "    commit; \n",
    "\n",
    "    \"\"\".format(schema=schema,begin_range=begin_range,end_range=end_range)\n",
    "    \n",
    "        cursor.execute(sql_script)\n",
    "    else:\n",
    "        print('Table already generated')\n",
    "    \n",
    "    cursor.close()\n",
    "    #df_label = pd.read_sql('select * from {schema}.recidivism_labels_{begin_range}_{end_range}'.format(\n",
    "    #                                                                                schema=schema,\n",
    "    #                                                                                begin_range=begin_range,\n",
    "    #                                                                                end_range=end_range), conn)\n",
    "    \n",
    "      \n",
    "    return '{schema}.recidivism_labels_{begin_range}_{end_range}'.format(schema=schema,\n",
    "                                                                        begin_range=begin_range,\n",
    "                                                                        end_range=end_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can generate labels for two time periods, one starting on 2006 and another starting in 2011\n",
    "train_label = create_labels(train_date, prediction_horizon, conn, schema=schema, overwrite=True)\n",
    "test_label = create_labels(test_date, prediction_horizon, conn, schema=schema, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a label: 0 indicates *no recidivism*, 1 indicates that person did return to jail within the outcome period (beginning of 2006 to end 2010). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Generation\n",
    "*[Go back to Table of Contents](#table-of-contents)*\n",
    "\n",
    "Our preliminary features are the following\n",
    "\n",
    "- `nadmits` (Aggregation): Number of times someone has been addmitted to prison between 1989-2005. The more times someone has been to prison the more times they are likely continue to be arrested. \n",
    "\n",
    "- `age_first_admit` (Transformation): The age someone was first admitted to prison. This is calculated by subtracting their `birth_yr` from the year they were first admitted into prison. The idea behind creating this feature is that people who are younger when they are first arrested are more likely to be arrested again. \n",
    "\n",
    "- `gang` (Binary): A binary (true/false) indicator of whether the person has ever been in a gang. This feature takes whatever gang membership (e.g., VL, LK BPM) and encodes affiliation as 1 and no affiliation (NULL) as 0.  \n",
    "\n",
    "- `married` (Binary): If the `marstat` field is `M`, then `married` is 1, or else it is 0 for any other categories, such as Single (`S`), Divorced (`D`), Widowed (`W`). \n",
    "\n",
    "- `job` (Binary): Does the person have a job lined up when they are released from prison: `Y` (1) or `N` (0), using the field `empplanf`. \n",
    "\n",
    "- `vetf` (Binary): Is the individual a veteran? `Y` (1) or `N`(0).\n",
    "\n",
    "- (`drugalcf, drugcocf, drugampf, drugmarf, drugherf, drugpcpf, drughothf, drugunkf`) (Binary): Used alcohol, cocaine, marijuana, heroin, pcp, other, unknown, respectively. For each field, `1` (positive) or `0` (negative).\n",
    "\n",
    "- `educlvl` (Binary): `1` for graduated high school or above, `0` otherwise.(*Caution: I belive the meaning of the values of this field changed overtime*) \n",
    "\n",
    "- `crtfind` (Binary): `1` for habitual offender or repeated abuser of children, `0` otherwise (as determined by the court). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL script to make features: \n",
    "```\n",
    "-------------------------------------------------------------------------------\n",
    "-- create features ------------------------------------------------------------\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "-- number of admits\n",
    "drop table if exists feature_nadmits_1989_2005;\n",
    "create table feature_nadmits_1989_2005 as\n",
    "select docnbr, count(*) nadmits\n",
    "from ildoc_admit\n",
    "where docnbr in (select docnbr from recidivism_labels_2005_2010)\n",
    "and curadmyr between 1988 and 2005\n",
    "group by docnbr;\n",
    "\n",
    "--NOTE: There are about 7000 people that have no admit record. They likely entered\n",
    "--into the sytem before 1989. \n",
    "\n",
    "-- age at first arrest\n",
    "drop table if exists docnbr_admityr;\n",
    "\n",
    "create temp table docnbr_admityr as\n",
    "select docnbr, min(curadmyr) min_admityr\n",
    "from ildoc_admit\n",
    "group by docnbr;\n",
    "\n",
    "-- join the table with the person birthyr columns\n",
    "drop table if exists age_first_admit_birth_year;\n",
    "\n",
    "create temp table age_first_admit_birth_year as\n",
    "select da.docnbr, da.min_admityr, p.birth_year\n",
    "from docnbr_admityr da\n",
    "left join person p on da.docnbr = p.ildoc_docnbr;\n",
    "\n",
    "--calculate the age at the first arrest \n",
    "\n",
    "drop table if exists feature_age_first_admit;\n",
    "\n",
    "create table feature_age_first_admit as\n",
    "select docnbr, (min_admityr - birth_year) age_first_admit\n",
    "from age_first_admit_birth_year;\n",
    "\n",
    "drop table if exists feature_agefirstadmit;\n",
    "create table feature_agefirstadmit as\n",
    "select docnbr, age_first_admit\n",
    "from feature_age_first_admit\n",
    "where docnbr in (select docnbr from feature_nadmits_1989_2005);\n",
    "\n",
    "-- categorical features\n",
    "drop table if exists categorical_features_1989_2005,\n",
    "                     cat_features_1989_2005;\n",
    "\n",
    "create temp table cat_features_1989_2005 as\n",
    "select a.*,\n",
    " to_date(LPAD(a.actmsrdt::text,8,'0'), 'MMDDYYYY') dactmsrdt,\n",
    " to_date(LPAD(a.actdisdt::text,8,'0'), 'MMDDYYYY') dactdisdt\n",
    "from ( select docnbr, gang, kids, marstat, empplanf, educlvl, vetf, crtfind1, drugalcf, drugcocf,\n",
    "drugampf, drugmarf, drugherf, drugpcpf, drugothf, drugunkf, actdisyr, actdisdt,\n",
    "actmsryr, actmsrdt\n",
    "from ildoc_exit\n",
    "where  docnbr in (select docnbr from last_exit_1989_2005)) a;\n",
    "\n",
    "--alter table cat_features_1989_2005 add column dactmsryr date; \n",
    "--update cat_features_1989_2005 set dactmsryr = to_date(LPAD(actmsrdt::text, 8,'0'));\n",
    "\n",
    "drop table if exists tcat_features_1989_2005;\n",
    "create temp table tcat_features_1989_2005 as\n",
    "select t.*,\n",
    "case when t.actdisyr is null or t.actdisyr = 0 then t.dactmsrdt else t.dactdisdt end release_dt\n",
    "from (select * from cat_features_1989_2005) t;\n",
    "\n",
    "\n",
    "alter table tcat_features_1989_2005\n",
    "drop column actdisyr,\n",
    "drop column actdisdt,\n",
    "drop column actmsryr,\n",
    "drop column actmsrdt,\n",
    "drop column dactmsrdt,\n",
    "drop column dactdisdt;\n",
    "\n",
    "drop table if exists pre_categorical_features_1989_2005;\n",
    "\n",
    "create temp table pre_categorical_features_1989_2005 as\n",
    "select g.*\n",
    "from ( select t.*, rank() over (partition by docnbr order by release_dt desc)\n",
    "from (select * from tcat_features_1989_2005) t ) g\n",
    "where g.rank = 1;\n",
    "\n",
    "drop table if exists raw_categorical_features_1989_2005;\n",
    "\n",
    "create temp table raw_categorical_features_1989_2005 as\n",
    "select distinct * from pre_categorical_features_1989_2005;\n",
    "\n",
    "drop table if exists categorical_features_1989_2005;\n",
    "\n",
    "create temp table pre_categorical_features_1989_2005 as\n",
    "select g.*\n",
    "from ( select t.*, rank() over (partition by docnbr order by release_dt desc)\n",
    "from (select * from tcat_features_1989_2005) t ) g\n",
    "where g.rank = 1;\n",
    "\n",
    "drop table if exists raw_categorical_features_1989_2005;\n",
    "\n",
    "create temp table raw_categorical_features_1989_2005 as\n",
    "select distinct * from pre_categorical_features_1989_2005;\n",
    "\n",
    "drop table if exists categorical_features_1989_2005;\n",
    "\n",
    "create table categorical_features_1989_2005 as\n",
    "select\n",
    "        docnbr,\n",
    "        case when gang is null then 0 else 1 end gang,\n",
    "        case when kids > 0 then 1 else 0 end kids,\n",
    "        case when marstat = 'M' then 1 else 0 end married,\n",
    "        case when empplanf = 'Y' then 1 else 0 end job,\n",
    "        case when educlvl in ('HS', 'GD','T1','T2','T3','T4','12','13',\n",
    "        '14','15','16','17') then 1\n",
    "              else 0 end educlvl,\n",
    "        case when crtfind1 in ('HB', 'HCO') then 1 else 0 end crtfind,\n",
    "        case when vetf = 'Y' then 1 else 0 end vetf,\n",
    "        case when drugalcf in ('F','X') then 1 else 0 end drugalcf,\n",
    "        case when drugcocf in ('F','X') then 1 else 0 end drugcocf,\n",
    "        case when drugampf in ('F','X') then 1 else 0 end drugampf,\n",
    "        case when drugmarf in ('F','X') then 1 else 0 end drugmarf,\n",
    "        case when drugherf in ('F','X') then 1 else 0 end drugherf,\n",
    "        case when drugpcpf in ('F','X') then 1 else 0 end drugpcpf,\n",
    "        case when drugothf in ('F','X') then 1 else 0 end drugothf,\n",
    "        case when drugunkf in ('F','X') then 1 else 0 end drugunkf\n",
    "from raw_categorical_features_1989_2005;\n",
    "\n",
    "-- create the age features \n",
    "drop table if exists feature_age_2005;\n",
    "create table feature_age_2005 as\n",
    "select ildoc_docnbr, (2005 - birth_year) age  from person\n",
    "where ildoc_docnbr in (select docnbr from feature_nadmits_1989_2005);\n",
    "\n",
    "-- join all features table together to make the feature table\n",
    "\n",
    "drop table if exists features_1989_2005;\n",
    "create table features_1989_2005 as\n",
    "select  f1.docnbr,\n",
    "        f1.nadmits,\n",
    "        f2.age_first_admit,\n",
    "        f3.gang,\n",
    "        f3.kids,\n",
    "        f3.married,\n",
    "        f3.job,\n",
    "        f3.vetf,\n",
    "        f3.drugalcf,\n",
    "        f3.drugcocf,\n",
    "        f3.drugampf,\n",
    "        f3.drugmarf,\n",
    "        f3.drugherf,\n",
    "        f3.drugpcpf,\n",
    "        f3.drugothf,\n",
    "        f3.drugunkf,\n",
    "        f3.educlvl,\n",
    "        f3.crtfind,\n",
    "        f4.age\n",
    "from feature_nadmits_1989_2005 f1\n",
    "left join feature_agefirstadmit f2 on f2.docnbr = f1.docnbr\n",
    "left join categorical_features_1989_2005 f3 on f1.docnbr = f3.docnbr\n",
    "left join feature_age_2005 f4 on f1.docnbr = f4.ildoc_docnbr;\n",
    "\n",
    "-- create training set to make cross-tabs with \n",
    "drop table if exists training_set_1989_2005;\n",
    "\n",
    "create table training_set_1989_2005 as\n",
    "select f1.*, r.recidivism from features_1989_2005 f1\n",
    "join recidivism_labels_2005_2010 r on f1.docnbr = r.docnbr\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(prediction_date, prediction_horizon, conn, schema='class1',overwrite=False):\n",
    "    \"\"\"\n",
    "    Generate a list of features and return the \n",
    "    table as a dataframe.\n",
    "    \n",
    "    Note: There has to be a table of labels that\n",
    "    correspond with the same time period. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction_date: int\n",
    "        year to make prediction from\n",
    "        (e.g., 2005 corresponds to Dec 31, 2005)\n",
    "    conn: obj\n",
    "        psycopg2 conection object to database\n",
    "    overwrite: bool\n",
    "        If True will run SQL script if tables\n",
    "        do not exist. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    table_name: str\n",
    "        name of table with features\n",
    "    \"\"\"\n",
    "    begin_range = prediction_date\n",
    "    end_range = prediction_date + prediction_horizon\n",
    "        \n",
    "    cursor = conn.cursor()\n",
    "    query = \"\"\"\n",
    "            select exists(select * from information_schema.tables \n",
    "            where table_name=\\'features_1989_{begin_range}\\')\n",
    "            \"\"\".format(begin_range=begin_range)\n",
    "    cursor.execute(query)\n",
    "    \n",
    "    if not(cursor.rowcount) or overwrite:\n",
    "\n",
    "    \n",
    "        sql_script=\"\"\"\n",
    "    -------------------------------------------------------------------------------\n",
    "    -- create features ------------------------------------------------------------\n",
    "    -------------------------------------------------------------------------------\n",
    "\n",
    "    -- for everyone in our labels table, we want to count number of\n",
    "    -- times they've been admitted as of the prediction_date\n",
    "\n",
    "    drop table if exists {schema}.feature_nadmits_1989_{begin_range};\n",
    "    create table {schema}.feature_nadmits_1989_{begin_range} as \n",
    "    select docnbr, count(*) nadmits\n",
    "    from ildoc.ildoc_admit\n",
    "    where docnbr in (select docnbr from {schema}.recidivism_labels_{begin_range}_{end_range})\n",
    "    and curadmyr > 1988 and curadmyr < {begin_range}\n",
    "    group by docnbr;  \n",
    "\n",
    "    commit; \n",
    "    --NOTE: There are about ### people that have no admit record. They likely entered\n",
    "    --into the sytem before 1989. \n",
    "\n",
    "    -- age at first arrest\n",
    "\n",
    "    -- step 1: get date of first admit\n",
    "    drop table if exists docnbr_admityr;\n",
    "    create temp table docnbr_admityr as\n",
    "    select docnbr, min(curadmyr) min_admityr\n",
    "    from ildoc.ildoc_admit\n",
    "    group by docnbr;\n",
    "\n",
    "    commit; \n",
    "\n",
    "    -- join the date of first admit to the table with the person birthyr columns\n",
    "    drop table if exists age_first_admit_birth_year;\n",
    "    create temp table age_first_admit_birth_year as\n",
    "    select da.docnbr, da.min_admityr, p.birth_year\n",
    "    from docnbr_admityr da\n",
    "    left join {schema}.person p on da.docnbr = p.ildoc_docnbr;\n",
    "\n",
    "    commit; \n",
    "\n",
    "    --now we can calculate the age at the first admit \n",
    "\n",
    "    drop table if exists {schema}.feature_age_first_admit; \n",
    "    create table {schema}.feature_age_first_admit as\n",
    "    select docnbr, (min_admityr - birth_year) age_first_admit\n",
    "    from age_first_admit_birth_year;\n",
    "\n",
    "    commit; \n",
    "\n",
    "    drop table if exists {schema}.feature_agefirstadmit; \n",
    "    create table {schema}.feature_agefirstadmit as \n",
    "    select docnbr, age_first_admit\n",
    "    from {schema}.feature_age_first_admit\n",
    "    where docnbr in (select docnbr from {schema}.feature_nadmits_1989_{begin_range}); \n",
    "\n",
    "    commit; \n",
    "\n",
    "    -- now let's create some categorical features\n",
    "    drop table if exists categorical_features_1989_{begin_range},\n",
    "                     cat_features_1989_{begin_range};\n",
    "    create temp table cat_features_1989_{begin_range} as\n",
    "    select a.*, \n",
    "     to_date(LPAD(a.actmsrdt::text,8,'0'), 'MMDDYYYY') dactmsrdt,\n",
    "     to_date(LPAD(a.actdisdt::text,8,'0'), 'MMDDYYYY') dactdisdt\n",
    "    from ( select docnbr, gang, kids, marstat, empplanf, educlvl, vetf, crtfind1, drugalcf, drugcocf,\n",
    "    drugampf, drugmarf, drugherf, drugpcpf, drugothf, drugunkf, actdisyr, actdisdt,\n",
    "    actmsryr, actmsrdt\n",
    "    from ildoc.ildoc_exit\n",
    "    where  docnbr in (select docnbr from last_exit_1989_{begin_range})) a;\n",
    "\n",
    "    commit; \n",
    "\n",
    "\n",
    "\n",
    "    drop table if exists tcat_features_1989_{begin_range}; \n",
    "    create temp table tcat_features_1989_{begin_range} as \n",
    "    select t.*,\n",
    "    case when t.actdisyr is null or t.actdisyr = 0 then t.dactmsrdt else t.dactdisdt end release_dt\n",
    "    from (select * from cat_features_1989_{begin_range}) t; \n",
    "\n",
    "    alter table tcat_features_1989_{begin_range} \n",
    "    drop column actdisyr,\n",
    "    drop column actdisdt, \n",
    "    drop column actmsryr,\n",
    "    drop column actmsrdt,\n",
    "    drop column dactmsrdt,\n",
    "    drop column dactdisdt; \n",
    "\n",
    "    commit; \n",
    "\n",
    "    drop table if exists pre_categorical_features_1989_{begin_range}; \n",
    "    create temp table pre_categorical_features_1989_{begin_range} as \n",
    "    select g.* \n",
    "    from ( select t.*, rank() over (partition by docnbr order by release_dt desc)\n",
    "    from (select * from tcat_features_1989_{begin_range}) t ) g\n",
    "    where g.rank = 1;\n",
    "\n",
    "    commit; \n",
    "\n",
    "    drop table if exists raw_categorical_features_1989_{begin_range}; \n",
    "    create temp table raw_categorical_features_1989_{begin_range} as\n",
    "    select distinct * from pre_categorical_features_1989_{begin_range}; \n",
    "\n",
    "    commit; \n",
    "\n",
    "    drop table if exists {schema}.categorical_features_1989_{begin_range}; \n",
    "    create table {schema}.categorical_features_1989_{begin_range} as \n",
    "    select\n",
    "        docnbr,  \n",
    "        case when gang is null then 0 else 1 end gang,\n",
    "        case when kids > 0 then 1 else 0 end kids,\n",
    "        case when marstat = 'M' then 1 else 0 end married,\n",
    "        case when empplanf = 'Y' then 1 else 0 end job,\n",
    "        case when educlvl in ('HS', 'GD','T1','T2','T3','T4','12','13',\n",
    "        '14','15','16','17') then 1\n",
    "              else 0 end educlvl,\n",
    "        case when crtfind1 in ('HB', 'HCO') then 1 else 0 end crtfind,\n",
    "        case when vetf = 'Y' then 1 else 0 end vetf,\n",
    "        case when drugalcf in ('F','X') then 1 else 0 end drugalcf,\n",
    "        case when drugcocf in ('F','X') then 1 else 0 end drugcocf,\n",
    "        case when drugampf in ('F','X') then 1 else 0 end drugampf,\n",
    "        case when drugmarf in ('F','X') then 1 else 0 end drugmarf,\n",
    "        case when drugherf in ('F','X') then 1 else 0 end drugherf,\n",
    "        case when drugpcpf in ('F','X') then 1 else 0 end drugpcpf,\n",
    "        case when drugothf in ('F','X') then 1 else 0 end drugothf,\n",
    "        case when drugunkf in ('F','X') then 1 else 0 end drugunkf\n",
    "    from raw_categorical_features_1989_{begin_range}; \n",
    "\n",
    "    commit; \n",
    "\n",
    "    -- create the age features \n",
    "    drop table if exists {schema}.feature_age_{begin_range}; \n",
    "    create table {schema}.feature_age_{begin_range} as\n",
    "    select ildoc_docnbr, ({begin_range} - birth_year) age  from {schema}.person \n",
    "    where ildoc_docnbr in (select docnbr from {schema}.feature_nadmits_1989_{begin_range});\n",
    "\n",
    "    commit; \n",
    "\n",
    "    -- join all features table together to make the feature table\n",
    "    drop table if exists {schema}.features_1989_{begin_range}; \n",
    "    create table {schema}.features_1989_{begin_range} as \n",
    "    select \tf1.docnbr,\n",
    "            f1.nadmits,\n",
    "            f2.age_first_admit,\n",
    "            f3.gang,\n",
    "        f3.kids,\n",
    "        f3.married,\n",
    "        f3.job,\n",
    "        f3.vetf,\n",
    "        f3.drugalcf,\n",
    "        f3.drugcocf,\n",
    "        f3.drugampf,\n",
    "        f3.drugmarf,\n",
    "        f3.drugherf,\n",
    "        f3.drugpcpf,\n",
    "        f3.drugothf,\n",
    "        f3.drugunkf,\n",
    "        f3.educlvl,\n",
    "        f3.crtfind,\n",
    "        f4.age \n",
    "    from {schema}.feature_nadmits_1989_{begin_range} f1\n",
    "    left join {schema}.feature_agefirstadmit f2 on f2.docnbr = f1.docnbr\n",
    "    left join {schema}.categorical_features_1989_{begin_range} f3 on f1.docnbr = f3.docnbr\n",
    "    left join {schema}.feature_age_{begin_range} f4 on f1.docnbr = f4.ildoc_docnbr; \n",
    "\n",
    "    commit; \n",
    "\n",
    "\n",
    "    \"\"\".format(schema=schema, begin_range=begin_range, end_range = end_range)\n",
    "    \n",
    "    \n",
    "        cursor.execute(sql_script)\n",
    "    \n",
    "    cursor.close()\n",
    "    \n",
    "    print('created {schema}.features_1989_{begin_range}'.format(\n",
    "        schema=schema,begin_range=begin_range))\n",
    "    \n",
    "    table_name = '{schema}.features_1989_{begin_range}'.format(schema=schema,begin_range=begin_range)\n",
    "    return table_name      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_table = create_features(train_date,prediction_horizon, conn, schema=schema, overwrite=True)\n",
    "test_feature_table = create_features(test_date, prediction_horizon ,conn, schema=schema, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Validation Data Sets\n",
    "*[Go back to Table of Contents](#table-of-contents)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Training Set\n",
    "\n",
    "We are going to create a training set that will take people at the beginning of 2006 and will generate labels for them based on data from 2006-2010. The features for each person are created based on data from the beginnig of our  data (1989) up to the end of 2005.\n",
    "\n",
    "*Note: it is important to segregate your data based on time when creating features. Otherwise there can be \"leakage,\" where you accidentally use information that you would not have known at the time.*  This happens often when calculating aggregation features; for instance, it is quite easy to calculate an average using values that go beyond our training set time-span and not realize it.  \n",
    "\n",
    "### Our Test (Validation) Set\n",
    "\n",
    "We will then take the model built on that training set and validate it on the Test Set. Our testing set will use labels from 2011-2015, and our features will be generated from 1989-2010. \n",
    "\n",
    "### Let's now create them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a matrix for training the models\n",
    "\n",
    "def create_train_or_test_matrix(feature_table_name,\n",
    "                                labels_table_name, \n",
    "                                matrix_name,\n",
    "                                conn,\n",
    "                                schema='class1',\n",
    "                                overwrite=False):\n",
    "    \"\"\"\n",
    "    joins feature table with the labels table to generate a matrix\n",
    "      \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        conn: obj\n",
    "        psycopg2 conection object to database\n",
    "    overwrite: bool\n",
    "        If True will run SQL script if tables\n",
    "        do not exist. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    table\n",
    "        table with features\n",
    "    \"\"\"\n",
    "        \n",
    "    cursor = conn.cursor()\n",
    "    query = \"\"\"\n",
    "            select * from information_schema.tables \n",
    "            where table_name=\\'{matrix_name}\\';\n",
    "            \"\"\".format(matrix_name=matrix_name)\n",
    "    cursor.execute(query)\n",
    "    \n",
    "    if not(cursor.rowcount) or overwrite:\n",
    "\n",
    "        sql_script=\"\"\"\n",
    "    drop table if exists {schema}.{matrix_name};\n",
    "    commit;\n",
    "    create table {schema}.{matrix_name} as \n",
    "    select f1.*, r.recidivism from {feature_table_name} f1\n",
    "    join {labels_table_name} r on f1.docnbr = r.docnbr; \n",
    "    commit; \n",
    "\n",
    "    \"\"\".format(matrix_name=matrix_name,\n",
    "               feature_table_name=feature_table_name, \n",
    "               labels_table_name = labels_table_name,\n",
    "               schema=schema)\n",
    "    \n",
    "    \n",
    "        cursor.execute(sql_script)\n",
    "    \n",
    "    cursor.close()\n",
    "    \n",
    "    print('created {schema}.{matrix_name}'.format(\n",
    "        matrix_name=matrix_name, schema=schema))\n",
    "    \n",
    "    return '{schema}.{matrix_name}'.format(schema=schema, matrix_name=matrix_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = create_train_or_test_matrix(train_feature_table,\n",
    "                                            train_label,\n",
    "                                            'train_matrix_{}'.format(train_date),\n",
    "                                            conn,\n",
    "                                            schema=schema,\n",
    "                                            overwrite=True)\n",
    "\n",
    "test_matrix = create_train_or_test_matrix(test_feature_table,\n",
    "                                        test_label,\n",
    "                            'test_matrix_{}'.format(test_date),\n",
    "                            conn,\n",
    "                            schema=schema,\n",
    "                            overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = pd.read_sql('select * from {}'.format(train_matrix), conn)\n",
    "df_testing = pd.read_sql('select * from {}'.format(test_matrix), conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isnan_training_rows = df_training.isnull().any(axis=1) # Find the rows where there are NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training[isnan_training_rows].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of `NaNs` in the `age_first_admit` and `age` field due to there being no `birth_yr` value in the `Person` table. Let's see how much of the data is missing these values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrows_training = df_training.shape[0]\n",
    "nrows_training_isnan = df_training[isnan_training_rows].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%of frows with NaNs {} '.format(float(nrows_training_isnan)/nrows_training))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3% of our datset is not too bad, we can just drop these rows and still have plenty of data. As long as we believe that these values are *missing at random*, i.e., we think that the missing `age_first_admit` is due to a data or clerical error and not because people who are more likely to reoffend are more likely to have missing values of `age_first_admit`, it's OK to drop these rows. In other situations we can \"impute\" the missing data (i.e, fill in the blanks in a principled and statistically sound way).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training = df_training[~isnan_training_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the values of the ages at first admit are reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique( df_training['age_first_admit'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! It is unlikely there are one-year-old infants being booked into jail! This is likely due to an incorrect entry in the `birth_yr` in the `Person` table. On the other end of the age spectrum, the ages are more likely to be correct, but this is still something that you'd want to do a \"sanity check\" on with someone who knows the data well.\n",
    "\n",
    "Let's drop any rows that have age 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = ~(df_training['age_first_admit'] == 1)\n",
    "df_training = df_training[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how much data we still have and how many examples of recidivism are in our training dataset. We don't necessarily need to have a perfect balance of recidivists and non-recivists, but it's good to know what the \"baseline\" is in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows: {}'.format(df_training.shape[0]))\n",
    "df_training['recidivism'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have about 200,000 examples, and about 25% of those are *positive* examples (recidivist), which is what we're trying to identify. About 75% of the examples are *negative* examples (non-recidivst)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isnan_testing_rows = df_testing.isnull().any(axis=1) # Find the rows where there are NaNs\n",
    "nrows_testing = df_testing.shape[0]\n",
    "nrows_testing_isnan = df_testing[isnan_testing_rows].shape[0]\n",
    "print('%of rows with NaNs {} '.format(float(nrows_testing_isnan)/nrows_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that about 2% of the rows in our testing set have missing values. This matches what we'd expect based on what we saw in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing[isnan_testing_rows].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not a good idea to do in practice so we would impute in some way\n",
    "df_testing = df_testing[~isnan_testing_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = ~(df_testing['age_first_admit'] == 1)\n",
    "df_testing = df_testing[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows: {}'.format(df_testing.shape[0]))\n",
    "df_testing['recidivism'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into features and labels\n",
    "Here we can decide which features/predictors to use in our model and what variable to use as the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sel_features = ['nadmits', 'age_first_admit', 'gang', 'kids', 'married', 'job', 'vetf',\n",
    "               'drugalcf', 'drugcocf', 'drugampf', 'drugmarf', 'drugherf', 'drugpcpf',\n",
    "               'drugothf', 'drugunkf', 'educlvl', 'crtfind', 'age']\n",
    "sel_label = 'recidivism'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use conventions typically used in python scikitlearn\n",
    "\n",
    "X_train = df_training[sel_features].values\n",
    "y_train = df_training[sel_label].values\n",
    "X_test = df_testing[sel_features].values\n",
    "y_test = df_testing[sel_label].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation \n",
    "*[Go back to Table of Contents](#table-of-contents)*\n",
    "\n",
    "In this phase, you take the predictors from your test set and apply your model to them, then assess the quality of the model by comparing the *predicted values* to the *actual values* for each record in your testing data set. \n",
    "\n",
    "- **Performance Estimation**: How well will our model do once it is deployed and applied to new data?\n",
    "\n",
    "Now let's use the model we just fit to make predictions on our test dataset, and see what our accuracy score is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fit a model\n",
    "from sklearn import linear_model\n",
    "model = linear_model.LogisticRegression(penalty='l1', C=1e5)\n",
    "model.fit( X_train, y_train )\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we print the model results, we see different parameters we can adjust as we refine the model based on running it against test data (values such as `intercept_scaling`, `max_iters`, `penalty`, and `solver`).  Example output:\n",
    "\n",
    "    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
    "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0)\n",
    "\n",
    "To adjust these parameters, one would alter the call that creates the `LogisticRegression()` model instance, passing it one or more of these parameters with a value other than the default.  So, to re-fit the model with `max_iter` of 1000, `intercept_scaling` of 2, and `solver` of \"lbfgs\" (pulled from thin air as an example), you'd create your model as follows:\n",
    "\n",
    "    model = LogisticRegression( max_iter = 1000, intercept_scaling = 2, solver = \"lbfgs\" )\n",
    "\n",
    "The basic way to choose values for, or \"tune,\" these parameters is the same as the way you choose a model: fit the model to your training data with a variety of parameters, and see which perform the best on the test set. An obvious drawback is that you can also *overfit* to your test set; in this case, you can alter your method of cross-validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Understanding\n",
    "\n",
    "Let's look at what the model learned and what the coefficients are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print \"The coefficients for each of the features are \" \n",
    "zip(sel_features, model.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " \n",
    "std_coef = np.std(X_test,0)*model.coef_\n",
    "zip(sel_features, std_coef[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation \n",
    "\n",
    "Machine learning models usually do not produce a prediction (0 or 1) directly. Rather, models produce a score (that can sometimes be interpreted a a probabilty) between 0 and 1, which lets you more finely rank all of the examples from *most likely* to *least likely* to have label 1 (positive). This score is then turned into a 0 or 1 based on a user-specified threshold. For example, you might label all examples that have a score greater than 0.5 (1/2) as positive (1), but there's no reason that has to be the cutoff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  from our \"predictors\" using the model.\n",
    "y_scores = model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of scores and see if it makes sense to us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_scores, kde=False, rug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our distribution of scores is skewed, with the majority of scores on the lower end of the scale. We expect this because 75% of the data is made up of nonrecidivists, so we'd guess that a higher proportion of the examples in the test set will be negative (meaning they should have lower scores). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_testing['y_score'] = y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing[['docnbr', 'y_score']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools like `sklearn` often have a default threshold of 0.5, but a good threshold is selected based on the data, model and the specific problem you are solving. As a trial run, let's set a threshold of 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_threshold = lambda x,y: 0 if x < y else 1 \n",
    "predicted = np.array( [calc_threshold(score,0.5) for score in y_scores] )\n",
    "expected = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "Once we have tuned our scores to 0 or 1 for classification, we create a *confusion matrix*, which  has four cells: true negatives, true positives, false negatives, and false positives. Each data point belongs in one of these cells, because it has both a ground truth and a predicted label. If an example was predicted to be negative and is negative, it's a true negative. If an example was predicted to be positive and is positive, it's a true positive. If an example was predicted to be negative and is positive, it's a false negative. If an example was predicted to be positive and is negative, it's a false negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(expected,predicted)\n",
    "print conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count of true negatives is `conf_matrix[0,0]`, false negatives `conf_matrix[1,0]`, true positives `conf_matrix[1,1]`, and false_positives `conf_matrix[0,1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate an accuracy score by comparing expected to predicted.\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(expected, predicted)\n",
    "print( \"Accuracy = \" + str( accuracy ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an accuracy score of 81%. Recall that our testing dataset had 80% non-recidivists and 18% recidivists. If we had just labeled all the examples as negative and guessed non-recidivist every time, we would have had an accuracy of 80%, so our basic model is not doing much better than a \"dumb classifier.\" That's ok, because we're just getting started!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision = precision_score(expected, predicted)\n",
    "recall = recall_score(expected, predicted)\n",
    "print( \"Precision = \" + str( precision ) )\n",
    "print( \"Recall= \" + str(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we care about our whole precision-recall space, we can optimize for a metric known as the **area under the curve (AUC-PR)**, which is the area under the precision-recall curve. The maximum AUC-PR is 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall(y_true,y_score):\n",
    "    \"\"\"\n",
    "    Plot a precision recall curve\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: ls\n",
    "        ground truth labels\n",
    "    y_score: ls\n",
    "        score output from model\n",
    "    \"\"\"\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true,y_score)\n",
    "    plt.plot(recall_curve, precision_curve)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    auc_val = auc(recall_curve,precision_curve)\n",
    "    print('AUC-PR: {0:1f}'.format(auc_val))\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall(expected, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall at k%\n",
    "\n",
    "If we only care about a specific part of the precision-recall curve we can focus on more fine-grained metrics. For instance, say there is a special program for people likely to be recidivists, but only 5% can be admitted. In that case, we would want to prioritize the 5% who were *most likely* to end up back in jail, and it wouldn't matter too much how accurate we were on the 80% or so who weren't very likely to end up back in jail. \n",
    "\n",
    "Let's say that, out of the approximately 200,000 prisoners, we can intervene on 5% of them, or the \"top\" 10,000 prisoners (where \"top\" means highest predicted risk of recidivism). We can then focus on optimizing our **precision at 5%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall_n(y_true, y_prob, model_name):\n",
    "    \"\"\"\n",
    "    y_true: ls\n",
    "        ls of ground truth labels\n",
    "    y_prob: ls\n",
    "        ls of predic proba from model\n",
    "    model_name: str\n",
    "        str of model name (e.g, LR_123)\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    y_score = y_prob\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
    "    precision_curve = precision_curve[:-1]\n",
    "    recall_curve = recall_curve[:-1]\n",
    "    pct_above_per_thresh = []\n",
    "    number_scored = len(y_score)\n",
    "    for value in pr_thresholds:\n",
    "        num_above_thresh = len(y_score[y_score>=value])\n",
    "        pct_above_thresh = num_above_thresh / float(number_scored)\n",
    "        pct_above_per_thresh.append(pct_above_thresh)\n",
    "    pct_above_per_thresh = np.array(pct_above_per_thresh)\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(pct_above_per_thresh, precision_curve, 'b')\n",
    "    ax1.set_xlabel('percent of population')\n",
    "    ax1.set_ylabel('precision', color='b')\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(pct_above_per_thresh, recall_curve, 'r')\n",
    "    ax2.set_ylabel('recall', color='r')\n",
    "    ax2.set_ylim(0,1.05)\n",
    "    \n",
    "    name = model_name\n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, y_scores,k):\n",
    "    \n",
    "    threshold = np.sort(y_scores)[::-1][int(k*len(y_scores))]\n",
    "    y_pred = np.asarray([1 if i >= threshold else 0 for i in y_scores ])\n",
    "    return precision_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_n(expected,y_scores, 'LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_at_5 = precision_at_k(expected,y_scores, 0.01)\n",
    "print('Precision at 1%: {:.2f}'.format(p_at_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Pipeline\n",
    "*[Go back to Table of Contents](#table-of-contents)*\n",
    "\n",
    "When working on machine learning projects, it is a good idea to structure your code as a modular **pipeline**, which contains all of the steps of your analysis, from the original data source to the results that you report, along with documentation. This has many advantages:\n",
    "- **Reproducibility**. It's important that your work be reproducible. This means that someone else should be able\n",
    "to see what you did, follow the exact same process, and come up with the exact same results. It also means that\n",
    "someone else can follow the steps you took and see what decisions you made, whether that person is a collaborator, \n",
    "a reviewer for a journal, or the agency you are working with. \n",
    "- **Ease of model evaluation and comparison**.\n",
    "- **Ability to make changes.** If you receive new data and want to go through the process again, or if there are \n",
    "updates to the data you used, you can easily substitute new data and reproduce the process without starting from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "*[Go back to Table of Contents](#table-of-contents)*\n",
    "\n",
    "We have only scratched the surface of what we can do with our model. We've only tried one classifier (Logistic Regression), and there are plenty more classification algorithms in `sklearn`. Let's try them! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = {'RF': RandomForestClassifier(n_estimators=500, n_jobs=-1),\n",
    "       'ET': ExtraTreesClassifier(n_estimators=10, n_jobs=-1, criterion='entropy'),\n",
    "        'LR': LogisticRegression(penalty='l1', C=1e5),\n",
    "        'SGD':SGDClassifier(loss='log'),\n",
    "        'GB': GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=10),\n",
    "        'NB': GaussianNB()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sel_clfs = ['RF', 'ET', 'LR', 'SGD', 'GB', 'NB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_p_at_k = 0\n",
    "for clfNM in sel_clfs:\n",
    "    clf = clfs[clfNM]\n",
    "    clf.fit( X_train, y_train )\n",
    "    print clf\n",
    "    y_score = clf.predict_proba(X_test)[:,1]\n",
    "    predicted = np.array(y_score)\n",
    "    expected = np.array(y_test)\n",
    "    plot_precision_recall_n(expected,predicted, clfNM)\n",
    "    p_at_5 = precision_at_k(expected,y_score, 0.05)\n",
    "    if max_p_at_k < p_at_5:\n",
    "        max_p_at_k = p_at_5\n",
    "    print('Precision at 5%: {:.2f}'.format(p_at_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline \n",
    "\n",
    "It is important to check our model against a reasonable **baseline** to know how well our model is doing. Without any context, 83% accuracy can sound really great... but it's not so great when you remember that you could do almost that well by declaring everyone a non-recividist, which would be stupid (not to mention useless) model. \n",
    "\n",
    "A good place to start is checking against a *random* baseline, assigning every example a label (positive or negative) completely at random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_p_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_score = [random.uniform(0,1) for i in enumerate(y_test)] \n",
    "random_predicted = np.array( [calc_threshold(score,0.5) for score in random_score] )\n",
    "random_p_at_5 = precision_at_k(expected,random_predicted, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another good practice is checking against an \"expert\" or rule of thumb baseline. For example, say that talking to people at the DOC, you find that they think it's much more likely that someone who has been in prison multiple times already will reoffend. Then you should check that your classifier does better than just labeling everyone who has had multiple past admits as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recidivism_predicted = np.array([ 1 if nadmit > 1 else 0 for nadmit in df_testing.nadmits.values ])\n",
    "recidivism_p_at_5 = precision_at_k(expected,recidivism_predicted,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_non_recidivist = np.array([0 for nadmit in df_testing.nadmits.values])\n",
    "all_non_recidivist_p_at_5 = precision_at_k(expected, all_non_recidivist,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "fig, ax = plt.subplots(1, figsize=(22,12))\n",
    "sns.set_context(\"poster\", font_scale=1.25, rc={\"lines.linewidth\":2.25, \"lines.markersize\":8})\n",
    "sns.barplot(['Random','All Non-Recidivist', 'Recidivism','Model'],\n",
    "            [random_p_at_5, all_non_recidivist_p_at_5, recidivism_p_at_5, max_p_at_k],\n",
    "            palette=['#6F777D','#6F777D','#6F777D','#800000'])\n",
    "plt.ylim(0,1)\n",
    "plt.ylabel('precision at 5%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's explore some of the models we just built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore random forest RF\n",
    "sel_clfs\n",
    "clf = clfs[sel_clfs[0]]\n",
    "#clf = clfs[clfNM]\n",
    "print clf\n",
    "clf.fit( X_train, y_train )\n",
    "print clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see if we can make this look a little better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_\n",
    "std = np.std ([tree.feature_importances_ for tree in clf.estimators_],\n",
    "       axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print (\"Feature ranking\")\n",
    "for f in range(X_test.shape[1]):\n",
    "    print (\"%d. %s (%f)\" % (f + 1, sel_features[f], importances[indices[f]]))\n",
    "\n",
    "# plot \n",
    "plt.figure\n",
    "plt.title (\"Feature Importances\")\n",
    "plt.bar(range(X_test.shape[1]), importances[indices], color='r',\n",
    "      yerr=std[indices], align = \"center\")\n",
    "plt.xticks(range(X_test.shape[1]), sel_features, rotation=90)\n",
    "plt.xlim([-1, X_test.shape[1]])\n",
    "plt.show\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise \n",
    "\n",
    "Our model has just scratched the surface. Try the following: \n",
    "    \n",
    "- Create more features\n",
    "- Try more models\n",
    "- Try different parameters for your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "*[Go back to Table of Contents](#table-of-contents)*\n",
    "\n",
    "- Hastie et al.'s [The Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/) is a classic and is available online for free.\n",
    "- James et al.'s [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/), also available online, includes less mathematics and is more approachable.\n",
    "- Wu et al.'s [Top 10 Algorithms in Data Mining](http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
