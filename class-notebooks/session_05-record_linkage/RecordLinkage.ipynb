{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Linkage Adapted from [here](https://github.com/CSSIP-AIR/Big-Data-Workbooks/blob/master/08.%20Data%20Linkage/Record%20Linkage.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The goal of record linkage is to determine if pairs of records describe the same identity. This is important for removing duplicates from a data source or joining two separate data sources together. Record linkage also goes by the terms data matching, merge/purge, duplication detection, de-duping, reference matching, co-reference/anaphora in various fields. There are several approaches to record linkage that include exact matching, rule-based linking and probabilistic linking. An example of exact matching is joining records based on social security number. Rule-based matching involves applying a cascading set of rules that relect the domain knowledge of the records being linked. In probabilistic record linkage, linkage weights are calculated based on records and a threshold is applied to make a decision of whether to link records or not. This tutorial will cover preprocessing data, rule-based linkage and probabilitic linkage using the Felligi-Sunter model. \n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Load the data](#Load-the-Data)\n",
    "2. [Explore the data](#Data-Exploration)\n",
    "3. [Preprocess the Data](#Preprocess:-clean-Up-Names-and-separate-to-first-middle-and-last-name)\n",
    "4. [Explore Metrics](#String-Comparators)\n",
    "5. [Create Rule-Based Linking](#Create-A-Rule-Based-System-for-record-matching.)\n",
    "6. [Create Probabilistic Liking Using Felligi Sunter](#Probabilisic-Record-Linkage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "from __future__ import print_function\n",
    "from six.moves import zip, range\n",
    "import pandas as pd\n",
    "import jellyfish\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first dataset is all nsf grants awarded between 2010-2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nsf_awards = pd.read_csv('./Datasets/nsf_awards_2010-2012.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nsf_awards.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second dataset is a list of all employees in the UC System in 2011. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ucpay = pd.read_csv('./Datasets/ucpay2011.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ucpay.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are some redacted names from the data. Let's do some exploratory analysis on the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ucpay.year.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we expected all employees are from 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ucpay.campus.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ucpay.groupby('campus').size().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ucpay.title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_ucpay.title.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 2626 unique positions in the System. It is likely only a very small subset received grants from the NSF. Typically with the title of Professor, Postdoc, Research Professional etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ucpay.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove all the redeacted names from the UC dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_ucpay.name != \"***********\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ucpay[mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ucpay = df_ucpay[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ucpay.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we filtered out all the redacted names and can see the name field in the uc data is given in *lastname*, *firstname* *middle* format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_cols = ['ID','campus', 'name', 'title']\n",
    "df_ucpay = df_ucpay[sel_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ucpay.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter the nsf data to only display awards from CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_mask = df_nsf_awards['StateCode'] == 'CA'\n",
    "df_nsf_awards = df_nsf_awards[state_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nsf_awards.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What kind of the records do we want to match\n",
    "\n",
    "Given the name of an award match their record with the UC database to get their position and employee id. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess: clean Up Names and separate to first middle and last name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = df_ucpay.name.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_names(name):\n",
    "    \"\"\"\n",
    "    Splits names fields into first, middle and last names\n",
    "    and return lower case values. \n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    name: str\n",
    "        e.g., SHAPIRO, JORDAN ISAAC\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (first, middle, last): str\n",
    "        e.g., mark calvin anderson\n",
    "    \"\"\"\n",
    "    \n",
    "    #split on the comma do get the last name\n",
    "    name=name.lower()\n",
    "    ls_name = name.split(',')\n",
    "\n",
    "    last_name = ls_name[0]\n",
    "    first_middle_name = ls_name[1]\n",
    "    \n",
    "    #split by space to get the first and middle name\n",
    "    ls_first_middle_name = first_middle_name.split()\n",
    "    if len(ls_first_middle_name) > 1:\n",
    "        first_name = ls_first_middle_name[0]\n",
    "        middle_name = ls_first_middle_name[1]\n",
    "    else: \n",
    "        first_name = ls_first_middle_name[0]\n",
    "        middle_name = ''\n",
    "    return unicode(first_name.strip()), unicode(middle_name.strip()), unicode(last_name.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_cleaned_names = [split_names(name) for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_first, ls_middle, ls_last = zip(*ls_cleaned_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ucpay['first'] = ls_first\n",
    "df_ucpay['middle'] = ls_middle\n",
    "df_ucpay['last'] = ls_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ucpay.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we know have cleaned fields for the first, middle and last names. The NSF data only has first and last name fields so we only need the first and last name fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nsf_awards.dropna(subset=['FirstName','LastName'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop any rows that do not have entries in the FirstName and LastName field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nsf_awards['first'] = [unicode(name.lower()) for name in df_nsf_awards['FirstName'].values]\n",
    "df_nsf_awards['last'] = [unicode(name.lower()) for name in df_nsf_awards['LastName'].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note**: In python2 we have to explicitly tell Python we want a string to be encoding in unicode. In Python3 all strings \n",
    ">are by default unicode. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_nsf_awards.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String Comparators\n",
    "\n",
    "Now that we have cleaned data lets explore how to match the string fields. For continuous fields, comparision is simple, the absolute difference between the values can be taken as a measure of the closeness. For string comparisions it is a little bit more complex. One metric is the *edit distance*, the minimum number of edit distances to transform one string to another. In the case of how many insertions, deletions and substitutions to transform one string to antoher that is known as the *Levenshtein distance*. If you add transposing with adjacent letters that is known as the *Levenshtein-Damerau distance*. The *Jaro-Winkler* distance is a fast-to-compute distance metric that returns a normalized score between zero and one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StringComparators():\n",
    "    \"\"\"\n",
    "    Test various string comparators \n",
    "    \"\"\"\n",
    "\n",
    "    def test_levenshtein_distance():\n",
    "        assert jellyfish.levenshtein_distance('John', 'John') == 0\n",
    "        assert jellyfish.levenshtein_distance('Jon', \"John\") == 2\n",
    "        assert jellyfish.levenshtein_distance('Joseph', 'Joesph') == 1\n",
    "        \n",
    "    def test_damerau_levenshtein():\n",
    "        assert jellyfish.damerau_levenshtein_distance('Joseph', 'Joesph') == 1\n",
    "\n",
    "    def test_jaro_winklear():\n",
    "        assert (np.isclose(jellyfish.jaro_winkler('Joseph', 'Joesph'), 0.955555))\n",
    "        assert (np.isclose(jellyfish.jaro_winkler('Chris', 'Christoper'), 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets get the to 10 matching first names in the nsf database according to the jaro-winker score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_nsf_firstname = set( df_nsf_awards['first'].values ) #grab unique names from the nsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uc_names = df_ucpay['first'].values #grab the uc_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Comparison of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testname = unicode(uc_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_matching_first_name(testname, NUM_NAMES=10):\n",
    "    \"\"\"\n",
    "    get top 10 first names that match\n",
    "    \"\"\"\n",
    "    dict_name_pair = {}\n",
    "    for name in uc_names:\n",
    "        name = unicode(name)\n",
    "        dist = jellyfish.jaro_winkler(testname,name)\n",
    "        dict_name_pair[name] = dist\n",
    "\n",
    "    orddict_dict_name_pair = OrderedDict(\n",
    "                                sorted(dict_name_pair.items(), key=lambda x: x[1]))\n",
    "\n",
    "    ls_sorted_name = list(orddict_dict_name_pair.keys())\n",
    "\n",
    "\n",
    "    return ls_sorted_name[-NUM_NAMES:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testname,get_matching_first_name(testname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nm in uc_names[:25]:\n",
    "    testname = unicode(nm)\n",
    "    print(testname, get_matching_first_name(testname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create A Rule Based System for record matching. \n",
    "\n",
    "Let's try to merge data with the following rules. \n",
    "\n",
    "1. The first name Jaro-Winkler score has to be greater than 0.90\n",
    "2. The last name Jaro-Winkler score has to be greater then 0.90\n",
    "\n",
    "This rule essentially means that the names have to match with very minor typos. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_nsf_awards = df_nsf_awards[:10].to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rule_mask(nsf_first_name, \n",
    "                     nsf_last_name,\n",
    "                     df_ucpay,\n",
    "                     first_name_thresh=0.90,\n",
    "                     last_name_thresh=0.90):\n",
    "    \"\"\"\n",
    "    Returns a boolean array of records to match based on a\n",
    "    fixed threshold. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    (nsf_first_name, nsf_last_name): str\n",
    "        first and last name in the NSF dataset\n",
    "        \n",
    "    df_ucpay: DataFrame\n",
    "        DataFrame of the UC directory\n",
    "        \n",
    "    (first_name_thresh,last_name_thresh): int\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    jaro_mask: ls[bool]\n",
    "        boolean list of records to match\n",
    "    \"\"\"\n",
    "    compare_first = lambda x: jellyfish.jaro_winkler(nsf_first_name,x)\n",
    "    compare_last = lambda x: jellyfish.jaro_winkler(nsf_last_name,x)\n",
    "\n",
    "    jaro_first = df_ucpay['first'].map(compare_first) \n",
    "    jaro_last = df_ucpay['last'].map(compare_last)\n",
    "\n",
    "    jaro_mask = (jaro_first > first_name_thresh) & (jaro_last > last_name_thresh)\n",
    "    \n",
    "    return jaro_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_records(dict_nsf_awards, df_ucpay, f_create_rule_mask):\n",
    "    \"\"\"\n",
    "    match records from the nsf and uc datasets based on the fields 'first' and 'last' name\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    dict_nsf_awards: dict\n",
    "        dictionary of nsf awards\n",
    "    df_ucpay: DataFrame\n",
    "        DataFrame of UC employees\n",
    "    create_rule_mask: function\n",
    "        Function that takes a first name, last name and df_ucpay\n",
    "        and returns a Boolean array of whether or not to match \n",
    "        records\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_linked_data: DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    df_linked_data = pd.DataFrame()\n",
    "    for key in dict_nsf_awards.keys():\n",
    "        dict_test_row = dict_nsf_awards[key]\n",
    "    \n",
    "        nsf_first_name = dict_test_row['first']\n",
    "        nsf_last_name = dict_test_row['last']\n",
    "\n",
    "        jaro_mask = f_create_rule_mask(nsf_first_name, nsf_last_name, df_ucpay)\n",
    "    \n",
    "        df_matches = df_ucpay[jaro_mask]\n",
    "        if len(df_matches) == 0:\n",
    "            print('No Match: {} {}'.format(nsf_first_name,nsf_last_name))\n",
    "        for row in df_matches.iterrows():\n",
    "            dict_test_row['ID'] = row[1]['ID']\n",
    "            dict_test_row['campus'] = row[1]['campus']\n",
    "            dict_test_row['name'] = row[1]['name']\n",
    "            dict_test_row['title'] = row[1]['title']\n",
    "            df_linked_data = df_linked_data.append(dict_test_row, ignore_index=True)\n",
    "            \n",
    "    return df_linked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linked_data = match_records(dict_nsf_awards, df_ucpay, create_rule_mask )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_col = ['AwardId', 'CityName', 'FirstName', 'ID', 'LastName', 'Name', 'campus', 'title', 'first', 'last']\n",
    "df_linked_data[sel_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see 4 records in the NSF database had no matches in teh UC database. Also if we examine the output of the record linkage more closely we see we have a few false postives, Paul Davies, Joseph Pasquale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several shortcomings to this approach:\n",
    "    \n",
    "1. There is no threshold that can be adjusted for the proper tolerance of false postives and false negatives\n",
    "2. As you apply more and more rules is can become unclear what the combination of rules has on the final linkage\n",
    "3. Rules also often reflect the creators domain knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilisic Record Linkage \n",
    "\n",
    "The Fellegi-Sunter probablistic record linkage model compares selected similiar fields in two records and calculated a weighted probablity of being similar.  The algorithm is the following: two fields are first compared using a metric, in this case, the jaro-winkler algorithm. The jaro-winkler distance is then obtained from the jaro-winkler algorithm is then binned into a category -- exact match, close match and no match. The category the comparison falls into is then weighed with two distributions. The probability that the records are a match and the probablity that the record are a non-match are then calculated for each field. The log probablity of being a match or a non-match are then combined for each field respectively. The final score is then the probablity of being a match minus the probablity of being a non-match. If the final score is greater then a thershold, then the records are considered to match.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FellegiSunter():\n",
    "    \"\"\"\n",
    "    class to implement Fellegi Sunter model\n",
    "    \"\"\"\n",
    "    \n",
    "    m_weights = {'first_name': (0.01,0.14,0.85),\n",
    "                 'last_name': (0.01,0.09,0.90)}\n",
    "    \n",
    "    u_weights = {'first_name': (0.88,0.10,0.02),\n",
    "                 'last_name': (0.91,0.08,0.01)}\n",
    "    \n",
    "    \n",
    "    def fuzzy_match(self,name1,name2):\n",
    "        \"\"\"\n",
    "        Compares two strings using jaro-winker and\n",
    "        outputs and returns one of three match\n",
    "        levels.\n",
    "        \n",
    "        * exact match is a jaro-winkler score >= 0.92\n",
    "        * close match is a jaro-winkler score > 0.85\n",
    "        * no match is a jaro-winkler score < 0.85\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        (name1, name2): str\n",
    "            two text strings to output\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        match_level: int\n",
    "            one of three match levels\n",
    "            2 - exact match \n",
    "            1 - close match\n",
    "            0 - not a match\n",
    "        \"\"\"\n",
    "        score = jellyfish.jaro_winkler(name1,name2)\n",
    "        if score >= 0.92:\n",
    "            return 2\n",
    "        elif score > 0.85:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def match_score(self,record1,record2):\n",
    "        \"\"\"\n",
    "        computes the match score between a pair\n",
    "        of records\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        (record1, record2): tuple(str)\n",
    "            tuples of records to be compared\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        match_score: int\n",
    "        \n",
    "        Raises\n",
    "        ------\n",
    "        Exception\n",
    "            tuples need to be the same size \n",
    "        \"\"\"\n",
    "        if not(len(record1) == len(record2)):\n",
    "            raise Exception('records need to be same size')\n",
    "        \n",
    "        scores = [self.fuzzy_match(rec1,rec2) for rec1, rec2 in zip(record1, record2)]\n",
    "        \n",
    "        first_name_score, last_name_score = scores\n",
    "        \n",
    "        #grab the m and u weights\n",
    "        \n",
    "        first_name_m_weight = self.m_weights['first_name'][first_name_score]\n",
    "        first_name_u_weight = self.u_weights['last_name'][first_name_score]\n",
    "        \n",
    "        last_name_m_weight = self.m_weights['first_name'][last_name_score]\n",
    "        last_name_u_weight = self.u_weights['last_name'][last_name_score]\n",
    "        \n",
    "        log_prob_match = math.log(first_name_m_weight) + math.log(last_name_m_weight)\n",
    "        log_prob_umatch = math.log(first_name_u_weight) + math.log(last_name_u_weight)\n",
    "        \n",
    "        match_score = log_prob_match - log_prob_umatch\n",
    "        \n",
    "        return match_score\n",
    "        \n",
    "    def link_record(self, record1, record2, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Returns True if records should be linked\n",
    "        False otherwise.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        (record1, record2): tuple(str)\n",
    "            tuples of records to be compared. must\n",
    "            be the same length\n",
    "        threshold: int\n",
    "            threshold for linking or not\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        link: bool\n",
    "            bool on whether to link two records or not\n",
    "        \"\"\"\n",
    "        \n",
    "        match_score = self.match_score(record1,record2)\n",
    "        if match_score > threshold:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs = FellegiSunter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( fs.link_record(('Avishek','Kumar'), ('Avishek','Kumar')) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( fs.link_record( ('Avishek','Kumar'), ('Anup','Kumar') ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's take this new function for a spin\n",
    "print('jonathon', 'jonthon',fs.fuzzy_match('jonathon','jonthon') )\n",
    "print('john', 'mark',fs.fuzzy_match('john','mark') )\n",
    "print('fred', 'frederick', fs.fuzzy_match('fred', 'frederick'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_jaro_mask(nsf_first_name, nsf_last_name, df_ucpay):\n",
    "    \"\"\"\n",
    "    create a boolean array for whether to link records based on\n",
    "    the jaro-winkler distance\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    (nsf_first_name, nsf_last_name): str\n",
    "        first and last name in the NSF dataset\n",
    "        \n",
    "    df_ucpay: DataFrame\n",
    "        DataFrame of the UC directory\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    jaro_mask: ls[bool]\n",
    "        boolean list of records to match\n",
    "    \"\"\"\n",
    "    record = (nsf_first_name, nsf_last_name)\n",
    "    uc_records = df_ucpay[['first','last']].values\n",
    "    \n",
    "    jaro_mask = [fs.link_record(record, uc_record) for uc_record in uc_records]\n",
    "    \n",
    "    return jaro_mask\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linked_data = match_records(dict_nsf_awards, df_ucpay, create_jaro_mask )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_col = ['AwardId', 'CityName', 'FirstName', 'ID', 'LastName', 'Name', 'campus', 'title', 'first', 'last']\n",
    "df_linked_data[sel_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the matching using probablistic matching. We can change the thresholds do see how results will vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Back to Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
