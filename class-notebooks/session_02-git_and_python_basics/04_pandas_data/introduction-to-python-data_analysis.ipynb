{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Python\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Intro to Python](#Intro-to-Python)\n",
    "  - [Introduction](#Introduction)\n",
    "  - [Python](#Python)\n",
    "  - [Python-Setup](#Python-Setup)\n",
    "  - [Basic-Plotting](#A-simple-example-of-plotting-of-using-numpy-and-plotting)\n",
    "  - [Getting help/documentation](#Getting-help/documentation)\n",
    "  - [Data-Basics](#Data-Basics)\n",
    "  - [Loading Data](#Loading-Data)\n",
    "  - [Understanding the Data](#Understanding-the-Data)\n",
    "  - [Value Counts](#value_counts)\n",
    "  - [Subplots](#Subplots)\n",
    "  - [Adding-columns](#Adding-columns)\n",
    "  - [Saving Results](#Saving-Results)\n",
    "4. [Exercises](#Exercises)\n",
    "5. [Resources](#Resources)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "This course is a practical introduction to the methods and tools that a social scientist can use to make sense of big data, and thus programming resources are also important. We make extensive use of the Python programming language and SQL database management. We recommend that any social scientist who aspires to work with large datasets become proficient in the use of these two systems as well as [Github](http://www.github.com). All three, fortunately, are quite accessible and supported by excellent online resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python\n",
    "---\n",
    "- Back to [Table of Contents](#table-of-contents)\n",
    "\n",
    "\n",
    "> Before coming to class, you should have completed the [DataCamp Intro to Python for Data Science](https://www.datacamp.com/courses/intro-to-python-for-data-science) course. It is free and takes about four hours. \n",
    "\n",
    "\n",
    "Python is a high-level interpreted general purpose programming language named after a British Comedy Troupe. Python was created by \n",
    "Guido van Rossum (Python's benovolent dictator fore life), and is maintained by an international group of enthusiasts. \n",
    "\n",
    "As of the time of this writing (10/2016) Python is currently the fifth most popular programming language. It is popular for data science because it is powerful and fast, it \"plays well\" with other languages, it runs everywhere, it's easy to learn, it's highly readable, open-source and its fast development time compared to other languages. Because of its general-purpose nature and its ability to call compiled languages like FORTRAN or C it can be used in full-stack development. There is a growing and always-improving list of open-source libraries for scientific programming, data manipulation, and data analysis (e.g., Numpy, Scipy, Pandas, Scikit-Learn, Statsmodels, Matplotlib, Seaborn, PyTables, etc.)\n",
    "\n",
    "[IPython](http://www.ipython.org) is an enhanced, interactive python interpreter that started as a grad school project by Fernando Perez. The project evolved into the IPython notebook, which allowed users to archive their code, figures, and analysis in a single document, making doing reproducible research and sharing said research much easier. The creators of the IPython notebook quickly realized that the \"notebook\" aspects were agnostic with respect to programming language, and ported the notebook to other languages including but not limited to Julia, Python and R. This then led to a rebranding known as the Jupyter Project. \n",
    "\n",
    "This tutorial will go over the basics of Data Analysis in Python using the PyData stack. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Python, we `import` packages. The `import` command allows us to use libraries created by others in our own work by \"importing\" them. You can think of importing a library as opening up a toolbox and pulling out a specific tool. \n",
    "- NumPy is short for numerical python. NumPy is a lynchpin in Python's scientific computing stack. Its strengths include a powerful *N*-dimensional array object, and a large suite of functions for doing numerical computing. \n",
    "- Pandas is a library in Python for data analysis that uses the DataFrame object from R which is similiar to a spreedsheet but allows you to do your analysis programaticaly rather than the point-and-click of Excel. It is a lynchpin of the PyData stack.  \n",
    "- Psycopg2 is a python library for interfacing with a PostGreSQL database. \n",
    "- Matplotlib is the standard plotting library in python. \n",
    "`%matlplotlib inline` is a so-called \"magic\" function of Jupyter that enables plots to be displayed inline with the code and text of a notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "from __future__ import print_function\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice we typically load libraries like `numpy` and `pandas` with shortened aliases, e.g, `import numpy as np`. This is like saying, \"`import numpy`, and wherever you see `np`, read it as `numpy`.\" Similarly, you'll often see `import pandas as pd`, or `import matplotlib.pyplot as plt`. \n",
    "\n",
    "Another shortcut is `%pylab inline`. This command includes both `import numpy as np` and `import matplotlib.pyplot as plt `. This shortcut was invented because it's faster to type `plt.plot()` rather than `matplotlib.pyplot.plot()`, and even programmers don't like to type more than they have to. \n",
    "\n",
    "In documentation and in examples, you will frequently see `numpy` commands starting with the alias `np` rather than `numpy` (e.g, `np.array()` or `np.argsort`) and `pandas` commands starting with `pd` (e.g., `pd.DataFrame()` or `pd.concat()`). See below for an example of using aliases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "%pylab inline\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from __future__ import print_function\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all `numpy` commands will be prefixed with `np` and all plotting commands will be prefixed with `plt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example of plotting using numpy and matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main workhorse library for plotting in Python is `matplotlib`. All commands in the `matplotlib` plotting library are stored in the `plt` namespace. A *namespace* contains all of the functions of that library grouped together, denoted by a common prefix. Any command using some function from the `matplotlib` plotting library will start with `plt`; for example, to create a plot we use the `plt.plot()` command. That way, if you've imported multiple libraries that each have a function called `plot()`, you know you're using the *right* one. Below is a very simple example of plotting a sine wave and adding labels to the axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0,4*np.pi,100) #the linspace command from the numpy library\n",
    "#creates a set of 100 equally spaced points from 0 to 4pi\n",
    "y = np.sin(x) #calculates the sin(x) which is stored in the y variable\n",
    "matplotlib.pyplot.plot(x,y) #plot x vs y \n",
    "matplotlib.pyplot.ylabel('sin(x)') #set the y-label\n",
    "matplotlib.pyplot.xlabel('x') # set the x-label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation and getting help\n",
    "\n",
    "Jupyter has great features for looking up the documentation of a function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the full documentation of a function, just type a question mark after the function name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an abbreviated documentation of a function, we can place the cursor in between the parentheses of a function and press `Shift+Tab` to get a shorter version of the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try it for yourself\n",
    "np.array()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Basics\n",
    "- Information about the data that we're using: where did it come from, what variables are present?\n",
    "- How to connect to the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll be using the [pandas package](http://pandas.pydata.org/) to read in and manipulate data. `pandas` reads data from the PostGreSQL database and stores the data in special table format called a \"dataframe,\" which will be familiar to you if you have used R or STATA for data analysis. Dataframes allow for easy statistical analysis, and can be directly used for machine learning. \n",
    "\n",
    "`pandas` uses a database engine to connect to databases. In the code cell below, we'll use `sqlalchemy` to connect to the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_name = \"appliedda\"\n",
    "db_host = \"10.10.2.10\"\n",
    "pgsql_connection = psycopg2.connect( host = db_host, database = db_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur = pgsql_connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pgsql_engine = sqlalchemy.create_engine( \"postgresql://10.10.2.10/appliedda\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "Next, we will use this database connection to have `pandas` retrieve the data. `pandas` has a set of [Input/Output tools](http://pandas.pydata.org/pandas-docs/stable/io.html) that let it read from and write to a large variety of tabular data formats, including CSV and Excel files, databases via SQL, JSON files, and even SAS and Stata data files. In the example below, we'll use the `pandas.read_sql()` function to read the results of an SQL query into a data frame.\n",
    "\n",
    "- We will change this query to only select the dataset that we want "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = 'SELECT * FROM {table};'.format(table=\"ildoc_admit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit = pd.read_sql( query, con = pgsql_engine )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see what the data looks like. The `pandas.DataFrame` method `head(number_of_rows)` outputs the first `number_of_rows` rows in a dataframe. Let's look at the first five rows in our data. Since our `pandas.DataFrame` object is called `df_ildoc_admit` and we want to see 5 rows, we'll type `df_ildoc_admit.head(5)`.\n",
    "\n",
    "In the code cells below, you'll see two ways to output this information. If you just call the method, you'll see an HTML table displayed directly in the IPython notebook. This is a useful way to display the information if you only want to view it in this notebook.  If you pass the results of the method to the `print()` function, you'll get text output. This is useful if you want to export the output for use outside the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to get a pretty tabular view, just call the method.\n",
    "df_ildoc_admit.head( 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df_ildoc_admit.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the *bottom* five rows. As you might be able to guess, `tail(number_of_rows)` does almost the same thing as `head(number_of_rows)`, but gives you the *last* `number_of_rows` rather than the *first* `number_of_rows`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to notice is that we have a pesky column called `unnamed:0`, which is just a duplicate of the index columns. Let's drop this column, since it has no purpose. We'll use the `drop()` method, and specify what we want to drop (`['unnamed: 0']`), which `axis` we're dropping it from (0 for rows, 1 for columns), and whether we want to make a *new* version of `df_ildoc_admit`, or just drop the column `inplace`. If you think you might want to be able to look back at the version of your dataframe before you made the alterations, you can specify `inplace=False`, but you will need to save the altered version under a new name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit.drop(['unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about the `drop` command and the arguments it uses, you can use our handy trick to read the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit.drop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Data \n",
    "In `pandas`, our data is represented by a `DataFrame`. You can think of dataframes as giant spreadsheets which you can *program*, rather than manipulating them with point-and-click tools like Excel. In a dataframe, each column is stored in its own *list*, which `pandas` calls a `Series` (or vector of values), along with a set of **methods** (another name for functions that are tied to objects) that make managing data easy.\n",
    "\n",
    "A `Series` is a list of values. Each value in a `Series` can have its own *label*, or `index`. If you retrieve a single *row* from your dataframe, it will come along with its `index`, or the names of the columns represented in each cell. If you retrieve a single *column* from your dataframe, the accompanying `index` will tell your the row IDs. \n",
    "\n",
    "While `DataFrame` and `Series` are separate objects, they share some of the same methods. In general, shared methods are those make sense in both a table and a list context (for example, `head()` and `tail()`, as seen in this notebook, can be used on both `DataFrame` and `Series` objects).\n",
    "\n",
    "More details on `pandas` data structures:\n",
    "- [Data Structures Overview](http://pandas.pydata.org/pandas-docs/stable/dsintro.html)\n",
    "- [Series specifics](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#series)\n",
    "- [DataFrame specifics](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With over 200 columns in our dataframe, it's too hard to look at them all at once. Let's get a list of the column names to see what we're working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, col in enumerate(df_ildoc_admit.columns):\n",
    "    print(i,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are 205 columns (although the last number is 204, remember that in Python we start counting from 0). It appears that a few of the columns might be redundant, like `birth_year` and `birth_yr`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to break down this code snippet.\n",
    "```\n",
    "for i, col in enumerate(df_ildoc_admit.columns):\n",
    "    print(i,col)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, `df_ildoc_admit.columns` gives us a list of the column names. We then *pass* that list to the `enumerate()` function, which returns a numerical index (`i`) and the name of the column (`col`) as the for loop goes through the list of columns. We print the values of `i` and `col` at each iteration using `print()`. Note that we can print multiple values (both `i` and `col`) so long as they are separated by a comma. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers of rows and columns are stored in the `shape` attribute of a `DataFrame`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if there are any duplicate rows in the dataframe by using the `drop_duplicates()` method and checking if that changes the shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit.drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, no duplicates!\n",
    "\n",
    "Here we ran the command `df_ildoc_admit.drop_duplicates()`, which returns a `DataFrame` with all duplicates removed, then accessed the `shape` of the *new* dataframe by just adding `.shape` after the `drop_duplicates()`. In many other languages, you would have to take these two steps individually, or nest the commands within each other in a way that makes your code hard to read. In this case, we did it all in one fell swoop, and it's easy to understand what we did, reading from left to right. The chaining of commands is a handy feature of python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start exploring the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned before, you can think of a `DataFrame` is an object made up of `Series` objects, which comprise its columns. Let's pull out just the `Series` for `'race'` and examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit['race'].head(10) #get first 10 entries of 'race' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit['race'].tail() #output the last five entries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the `tail()` example above we didn't *specify* how many rows we wanted, and we got 5, because that's the default for this method. It's good practice to read documentation, so you know the default values methods use for any of the possible arguments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting value counts \n",
    "\n",
    "Let's look at the distribution of `race` in our dataset.\n",
    "\n",
    "- **`value_counts()`** - The `value_counts()` \"series method and top-level function computes a histogram of a one-dimensional array of values.\" ( See [documentation](http://pandas.pydata.org/pandas-docs/stable/basics.html#value-counts-histogramming-mode) ).  This method returns a `Series` of the counts of the number of times each unique value in the column is present in the column (also known as frequencies), from largest count to least, with the value itself the label for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit['race'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are six distinct races represented. By far the most represented race is ####, which has more than twice as many entries as the next most common, white. Note that this does not necessarily mean that there are more than twice as many *people* who are ##### as ZZZZZZ represented in this dataset, because there could be multiple rows (i.e. multiple admissions) for an individual.\n",
    "\n",
    "Let's make a bar plot. `pandas` comes with a handy `plot()` function, which allows us to specify the type of plot we want using the keyword argument `kind`. \n",
    "\n",
    "When you give a function multiple *arguments*, the function can differentiate between the arguments by their order (**positional arguments**) or by their names (**keyword arguments**). \n",
    "\n",
    "A method like `head()` *expects* that the first argument you give it will be the number of rows you want, so you can get the first 10 rows of `df_ildoc_admit` by typing `df_ildoc_admit.head(10)`. This is an example of a positional argument. Keyword arguments have to be specified along with their name.  \n",
    "\n",
    "In the case of `pd.Series.plot()`, the keyword argument is `kind`, for the *kind* of plot we want. We pass the string `'bar'` (with single quotes) to indicate we would like to plot a barplot. Some other possible values would be `'line'` or `'box'`. Check out the documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit['race'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even create a horizontal barplot by passing the string `'barh'` into the `plot()` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit['race'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a bit more with keyword arguments. When we use the `value_counts()` command, we can normalize the results to get *percentages* (rather than absolute numbers) of prisoners using the keyword argument `normalize=True`, and sort the results from largest to smallest using the keyword argument `ascending=True`. Then we can plot the resulting `Series` as a horizontal bar plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit['race'].value_counts(normalize=True, ascending=True).plot(kind='barh', xlim=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, those that identify as $#$#$# make up a larger proportion of the prison population than those that identify as #$#$##. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to do the samething for the `sex` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit['sex'].value_counts(normalize=True, ascending=False).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly $$$ make up the majority of the prisoners in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subplots\n",
    "`Matplotlib` also comes with a handy subplot feature where we can plot multiple subplots and tune the display of a plot. \n",
    "\n",
    "Let's take the last two plots we made and plot them side-by-side, and make the figures bigger. When we invoke the `plt.subplots()` command, the first argument is the number of rows, `1`, and `2` is the number of columns. We then pass a tuple pair `(16,6)` of length and width (in inches) to the keyword argument `figsize`. The `plt.subplots()` function returns a tuple `fig` and `ax` object. The `fig` object controls *figure-level attributes* of the figure, like saving the file, while the `ax` object controls axis-level attributes for each figure respectively. We can create a plots as we did above and specify which axis object to plot them on using the `ax` keyword. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = matplotlib.pyplot.subplots(1,2, figsize=(16,6))\n",
    "df_ildoc_admit['race'].value_counts(normalize=True, ascending=True).plot(kind='barh', ax=ax[0])\n",
    "df_ildoc_admit['sex'].value_counts(normalize=True, ascending=True).plot(kind='barh', ax=ax[1])\n",
    "fig.savefig('fig1_race_gender_barh.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a challenge, let's try to make a subplot where the barplots are *horizontally* stacked and the x-axis ranges from 0 to 1 in both plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fg, ax = matplotlib.pyplot.subplots(2, figsize=(16,6), sharex=True)\n",
    "for i,pair in enumerate(df_ildoc_admit[['race','sex']].groupby('sex')):\n",
    "    label = pair[0]\n",
    "    df = pair[1]\n",
    "    df['race'].value_counts(normalize=True, ascending=True).plot(kind='barh', title=label, ax=ax[i], xlim=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that *within each sex*, the breakdown of race is generally the same.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding columns\n",
    "\n",
    "- Adding columns to a dataframe, using the map and lambda functions\n",
    "\n",
    "Now we'll calculate the approximate distribution of ages of people admitted into the corrections system (assuming that they are alive today). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda functions\n",
    "\n",
    "A **lambda function** is an anonymous function, or a function that we can use as a throw-away without explcitly naming it. Typically these are functions we only use once. \n",
    "\n",
    "In this case, we'll create a a simple function that calculates the difference in years between 2016 and another year. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "calc_age = lambda x : 2016 - x \n",
    "\n",
    "print(calc_age(1980))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with our new lambda function, we can use the **map function**, a method of the `Series` object, to map our function onto the entries in the `Series`. This allows us to use `calc_age()` on all of the entries in the `birth_year` column at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit['birth_year'].map(calc_age).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the ages, we can create a new field called `age` in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit['age'] = df_ildoc_admit['birth_year'].map(calc_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `describe()` method to get distributions of the ages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit['age'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a histogram of the ages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit['age'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, we see that the ages look roughly normally distributed, with the median age around XX, and a standard deviation of about XX years. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Results\n",
    "`Pandas` has methods for saving results in a variety of formats, from SQL dumps to Excel, JSON and CSV. Let's save our table as a CSV file using the `to_csv` method. We are going to pass the `index=False` parameter so we don't get those pesky `'Unnamed: 0'` columns when we reload it. \n",
    "\n",
    "The first argument we pass to `to_csv()` is what we want to call the file. By default, the file will be saved in the directory that you are working in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit.to_csv('ildoc_admit.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises \n",
    "---\n",
    "- Back to [Table of Contents](#table-of-contents)\n",
    "\n",
    "We have just scratched the surface of what can be done. Let's do a bit more data analysis!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a barplot of the escape risk \n",
    "\n",
    "The data contains a field `escrisk` which stands for escape risk. The categories are: \n",
    "\n",
    "\n",
    "* H: High\n",
    "* M: Moderate\n",
    "* L: Low\n",
    "* P: Pending\n",
    "\n",
    "Create a barplot of the data. Also include values that are listed as `NaN`. See the documentation on how to include this option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit['escrisk'].unique() #check what all the unique values are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit['escrisk'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit['escrisk'].value_counts(normalize=True, ascending=True, dropna=False).plot(kind='barh', xlim=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This excludes any row where the escrick is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the number of unique individuals in the data \n",
    "\n",
    "The IDOC number is a unique five digit number preceded by an alpha character which is assigned to an inmate at first reception to IDOC. \n",
    "\n",
    "1. First find how many entries there are in the table by how many ildoc numbers there are. \n",
    "2. Find how many unique ildoc numbers there are. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit['docnbr'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit.docnbr.unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the maximum number of admits a single prisoner has had in the ildoc system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's group the records by the `docnbr`, then count the number of records in each group.\n",
    "\n",
    "Sort the values from highest to least, and then get the top value. Which prisoner has the highest number of admits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_grpby_docnbr = df_ildoc_admit.groupby('docnbr') #use the groupby commmnd to group the records by ildoc number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_count_docnbr = df_grpby_docnbr.size() #use the size method to count the number of records in each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_count_docnbr.sort_values(ascending=False)[:1] #sort the values and then pull out the first value which is the highest one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you get more comfortable with pandas and python this can be done in a single line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit.groupby('docnbr').size().sort_values(ascending=False)[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the mean, median and minimum values for the admits of a single prisoner in the ildoc system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit.groupby('docnbr').size().sort_values(ascending=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit.groupby('docnbr').size().sort_values(ascending=False).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit.groupby('docnbr').size().sort_values(ascending=False).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the distribution of admits by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ildoc_admit['curadmyr'].value_counts().sort_values(ascending=True).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- Back to [Table of Contents](#table-of-contents)\n",
    "\n",
    "- Wes McKinney, the creator of pandas, wrote the standard text \"Data Analyis in Python: Data wrangling with Pandas, Numpy and IPython\"\n",
    "- Alex Bell's [Python for Economists](http://www.alexmbell.com/python-tutorial-for-economists/) provides a wonderful 30-page introduction to the use of Python in the social sciences, complete with XKCD cartoons. \n",
    "- Economists Tom Sargent and John Stachurski provide a [very useful set of lectures and examples](http://lectures.quantecon.org)\n",
    "- For more detail, we recommend Charles Severance's [Python for Informatics: Exploring Information](http://www.pythonlearn.com/book_007.pdf)\n",
    "- [Software Carpentry version control tutorial](https://swcarpentry.github.io/git-novice/)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
