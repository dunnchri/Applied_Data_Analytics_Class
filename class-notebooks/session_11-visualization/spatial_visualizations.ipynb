{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Introduction to Spatial Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Simple maps](#Simple-maps)\n",
    "2. [Choropleth](#Choropleths)\n",
    "2. [Choropleth of rate](#Choropleth-of-rate)\n",
    "3. [Choropleth multiple years](#Choropleth-multiple-years)\n",
    "\n",
    "[Addendum](#Addendum)\n",
    "1. Overview of [spatial data types](#Spatial-data-types)\n",
    "2. [Datasets](#Datasets) in this Notebook\n",
    "2. [Coordinate Reference Systems (aka projections)](#Coordinate-Reference-Systems)\n",
    "\n",
    "\n",
    "### Reference material\n",
    "**NOTE: much of the below material combines using SQL via Pandas and Geopandas to get the data in the format we want for spatial analyses or visualization in Python, As a reminder, the \"[Spatial Queries in PostGIS](../spatial_notebooks/Spatial-Queries-in-PostGIS.ipynb)\" has more detailed explanations of PostGIS and will help you better understand the underlying spatial SQL in this notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple maps\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We added a point file of prison locations, the `data_dir_1` below is just the location where that point file exists. The source for the data is in the [Datasets](#Datasets) section in the addendum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# directory of some sample data (IL prisons)\n",
    "data_dir_1 = '../../data/sample_data/'\n",
    "!ls {data_dir_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## data manipulation libraries ##\n",
    "# Pandas for generic manipulation\n",
    "import pandas as pd\n",
    "# GeoPandas for spatial data manipulation\n",
    "import geopandas as gpd\n",
    "\n",
    "# PySAL for spatial statistics\n",
    "import pysal as ps\n",
    "\n",
    "# shapely for specific spatial data tasks (GeoPandas uses Shapely objects)\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "\n",
    "# SQLAlchemy to get some data from the database\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# improve control of visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in point locations\n",
    "il_prisons = gpd.read_file('{}IL_prisons.shp'.format(data_dir_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# what info is contained in this file?\n",
    "il_prisons.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# what do the first rows look like?\n",
    "il_prisons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# what does a simple map of those locations look like? \n",
    "# Note that you can pass matplotlib keywords to (geo)pandas, like figsize\n",
    "il_prisons.plot(figsize=(4,6))\n",
    "plt.annotate('Source: Geocommons', xy=(0.8,-0.10), xycoords=\"axes fraction\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There's some dots. Not super useful, but we can see by the longitude (x axis) and latitude (y axis) they're where we'd expect for Illinois. \n",
    "\n",
    "Let's add IL counites too to give some context, to do so we'll pull the county polygons from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up connection paramaters for the 'appliedda' database\n",
    "host = '10.10.2.10'\n",
    "db = 'appliedda'\n",
    "\n",
    "# create DB connection \n",
    "# (connection string is \"<database-type>://<username>@<host-url>:<port>/<database-name>\")\n",
    "engine = create_engine(\"postgresql://{host_str}/{db_str}\".format(host_str = host, db_str=db))\n",
    "\n",
    "# create SQL query - limit to just IL by using the state FIPS code of 17\n",
    "sql = \"SELECT * FROM tl_2016_us_county WHERE statefp = '17';\"\n",
    "\n",
    "# get data from the database\n",
    "il_counties = gpd.read_postgis(sql, engine, geom_col='geom', \n",
    "                               index_col='gid', crs={'init': u'epsg:4269'})\n",
    "# note we need to include the \"geom_col\" and \"crs\" parameters\n",
    "# so that geopandas knows which column containes the geographic info\n",
    "# and the correct Coordinate Reference System of the data\n",
    "\n",
    "\n",
    "# see info of new geodataframe\n",
    "il_counties.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot counties - using all defaults in pandas's plot() function\n",
    "# adds colors essentially randomly based on order of shapes in the dataframe\n",
    "il_counties.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set CRS of to the same projection as counties\n",
    "# see more info about Coordinate Reference Systems (CRS) in the addendum\n",
    "il_prisons.to_crs(epsg=4269, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now let's try putting the point locations on top of counties, \n",
    "# and let's make counties grey\n",
    "\n",
    "# create a map of IL counties colored grey\n",
    "ax = il_counties.plot(color='grey', figsize=(6,8))\n",
    "# note we assign counties to the object \"ax\" so we can \n",
    "# overlay the points on the same \"matplotlib axis\"\n",
    "\n",
    "# use the same \"ax\" object to plot the prisons on top of the county map, \n",
    "# plus resize the markers and remove their outlines\n",
    "il_prisons.plot(ax=ax, markersize=10, markeredgewidth=0); \n",
    "plt.annotate('Source: Census TIGERLines & GeoCommons', xy=(0.8,-0.10), xycoords=\"axes fraction\")\n",
    "plt.title('IL State Prisons');\n",
    "# pro tip: adding this semi-colon at the end stops Jupyter \n",
    "# from printing out the \"<matplotlib.axes....>\" line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choropleths\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Choropleths are very useful because they can quickly convey how values compare across the study area. Let's start wtih a simple example of the land area of each county. (Note much of the code below comes from Sergio and Dani's Geovisualization notebook -> source: http://darribas.org/gds_scipy16/ipynb_md/02_geovisualization.html)\n",
    "\n",
    "Let's start with a simple dataset of number of ex-offenders by zipcode for 2015 in the Chicago Metro area.\n",
    "\n",
    "First, we're going to create the list of zipcodes we want by grabing only those that intersect counties in Chicago's Metro area (CBSA code 16980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# query for list of zipcodes:\n",
    "query = \"\"\"SELECT DISTINCT z.geoid10 as zipcode \n",
    "FROM tl_2016_us_zcta510 z JOIN tl_2016_us_county c \n",
    "ON z.geom && c.geom \n",
    "WHERE c.cbsafp = '16980'; \"\"\"\n",
    "\n",
    "# get the zipcodes as a pandas dataframe\n",
    "zipcodes = pd.read_sql(query, engine)\n",
    "zipcodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now convert the zipcode dataframe into a string \n",
    "# with single quotes and commas separating each\n",
    "# to select out just those zipcodes in the following SQL query\n",
    "zipcodes = ','.join([\"'\"+z+\"'\" for z in zipcodes.zipcode])\n",
    "\n",
    "# view first 23 characters to see if it looks right\n",
    "print(zipcodes[:23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get data - total ex-offenders by zipcode for 2015 joined to zipcode polygons\n",
    "# and we'll subset to only those zipcodes we want\n",
    "qry = \"SELECT z.gid, zip5, tot_exoff, geom \\\n",
    "FROM tl_2016_us_zcta510 AS z \\\n",
    "JOIN (SELECT zip5, count(docnbr) AS tot_exoff \\\n",
    "       FROM ildoc.ildoc_exit WHERE exityr = '2015' GROUP BY zip5) AS e \\\n",
    "ON z.geoid10 = e.zip5 WHERE z.geoid10 IN ({});\".format(zipcodes)\n",
    "\n",
    "exoff_zip = gpd.read_postgis(qry, engine, geom_col='geom', \n",
    "                             index_col='gid', crs='+init=epsg:4269')\n",
    "exoff_zip.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# what are the descriptive stats for this subset of zipcodes?\n",
    "exoff_zip.tot_exoff.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: since there are zipcodes that have less than 3 observations we will not print out individual records in this example workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check total exits from DOC\n",
    "print('total 2015 DOC exits in subset = {:,.0f}'.format(exoff_zip.tot_exoff.sum()))\n",
    "# and percent of those in top quartile\n",
    "print('percent of exits in top quartile = {:.2f}'.format(\n",
    "    100.*exoff_zip[exoff_zip.tot_exoff>=10].tot_exoff.sum()/exoff_zip.tot_exoff.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we'll create our matplotlib figure and axis first so we have more control over it later\n",
    "f, ax = plt.subplots(figsize=(6,8))\n",
    "\n",
    "# we'll pass the geopandas plot() function the column, scheme (calculation method), \n",
    "# number of groups to calculate (k)\n",
    "# colormap to use, linewidge (to make the edges less noticeable), and \n",
    "# the axis object created above\n",
    "exoff_zip.plot(column='tot_exoff', scheme='QUANTILES', k=10, \n",
    "               cmap='plasma', linewidth=0.1, ax=ax)\n",
    "\n",
    "# and this time we'll turn off the\n",
    "ax.set_axis_off();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As you can see, Geopandas only allows using the \"quantiles\" (or any other [scheme supported by PySAL](http://pysal.readthedocs.io/en/latest/library/esda/mapclassify.html)) with between 2 and 9 and if you try soemthing different, it resets to 5. You may also notice some \"holes\" in our map - these are zipcodes in which there are no ex-offenders from 2015.\n",
    "\n",
    "So here is how you can use more categories for your choropleths: create a new column with the appropriate PySAL function and map that, as follows. By \"appropriate PySAL function\" we mean whichever classification algorithm you select, above we used Quantiles and below we are using the Fisher Jenks algorithm. The PySAL documentation is quite good so we encourage you check there (as well as the notebook from Julia Koschinsky's colleagues referenced above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's try the 'Fisher_Jenks' scheme:\n",
    "fj10 = ps.Fisher_Jenks(exoff_zip.tot_exoff,k=10)\n",
    "\n",
    "# the ps.<scheme> function returns two things, the bins used for the cutoffs:\n",
    "print('bins:')\n",
    "print(fj10.bins)\n",
    "# and the assigned bin number to use:\n",
    "print('\\nbin number:')\n",
    "print(fj10.yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now we can use the new categories to color the choropleth of land area into 10 buckets\n",
    "# notice the couple new keywords we include\n",
    "\n",
    "# again we'll create the matplotlib figure and axis objects first\n",
    "f, ax = plt.subplots(figsize=(6, 8))\n",
    "\n",
    "# then create our choropleth, \n",
    "# the \"assign\" function adds our Fisher Jenks buckets as a new column to map\n",
    "## Note with this formulation 'cl' is a temporary column\n",
    "# the 'categorical=True' tells geopandas we want a different color for each category\n",
    "exoff_zip.assign(cl=fj10.yb).plot(column='cl', categorical=True, \\\n",
    "        cmap='viridis', linewidth=0.1, ax=ax, \\\n",
    "        edgecolor='grey', legend=True)\n",
    "\n",
    "# turn off the latitude/longitude axes\n",
    "ax.set_axis_off();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The above map highlights zipcodes of highest ex-offender populations, which appear to be in Chicago, however this brings us to an issue central to much of spatial analysis: do we actually care about the _total number of ex-offenders_? Maybe. Usually, however, we care more about **where there are _higher concentrations_ of our study variable as compared to some base value** so next we will consider the percentage of the local population that was released from prison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choropleth of rate\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "> If you would like to see how the zipcode population was generated for 2015 ACS data please see [this notebook](../../data/Tract-ACS-to-zipcode-conversion.ipynb), which includes writing the zipcode<->Tract relationship file and creating the zipcode level population estimates. \n",
    "\n",
    "First, we need to calculate the rate by zipcode from the ex-offender population and the total population for each zipcode - let's do that directly in the database using our same list of zipcodes above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# almost our same query from above, with two differences\n",
    "# 1. divide the total ex-offenders by total population, and\n",
    "# 2. do a LEFT JOIN to return all zipcodes in our list\n",
    "qry = \"SELECT z.gid, z.geoid10, zip5, tot_exoff/pop_est_2010::numeric exoffrate, geom \\\n",
    "FROM tl_2016_us_zcta510 AS z \\\n",
    "LEFT JOIN (SELECT zip5, count(docnbr) AS tot_exoff \\\n",
    "       FROM ildoc.ildoc_exit WHERE exityr = '2015' GROUP BY zip5) AS e \\\n",
    "ON z.geoid10 = e.zip5 \\\n",
    "WHERE z.geoid10 IN ({});\".format(zipcodes)\n",
    "\n",
    "# get new data\n",
    "ex_off_rate = gpd.read_postgis(qry, engine, geom_col='geom', index_col='gid')\n",
    "\n",
    "# view info of returned GeoDataFrame\n",
    "ex_off_rate.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# what are the descriptive stats of the rate column?\n",
    "ex_off_rate.exoffrate.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# but we also see that some in our initial pull are Nan, how many? \n",
    "print('zipcodes without observations %s' %\n",
    "      ex_off_rate[ex_off_rate.exoffrate.isnull()].shape[0])\n",
    "print('total zipcodes in Chicago Metro %s' % ex_off_rate.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace nulls with 0 as we assume there are no ex-offenders who were released in 2015\n",
    "ex_off_rate.exoffrate.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# again we'll use fisher-jenks with 10 classes\n",
    "fj10 = ps.Fisher_Jenks(ex_off_rate.exoffrate,k=10)\n",
    "\n",
    "# again we'll create the matplotlib figure and axis objects first\n",
    "f, ax = plt.subplots(figsize=(6, 8))\n",
    "\n",
    "# then create our choropleth, the \"assign\" function adds our \n",
    "# Fisher Jenks buckets as a new column to map\n",
    "## Note as before with this formulation 'cl' is a temporary column\n",
    "# the 'categorical=True' tells geopandas we want a different color for each category\n",
    "ex_off_rate.assign(cl=fj10.yb).plot(column='cl', categorical=True, \\\n",
    "        cmap='viridis_r', linewidth=0.1, ax=ax, \\\n",
    "        edgecolor='grey', legend=True)\n",
    "\n",
    "# let's add a title to describe what we're plotting, the '\\n' adds a new line\n",
    "my_title = 'Rate of ex-offenders in Chicago zip codes\\n'\n",
    "my_title += 'in 2015 categorized with Fisher Jenkes'\n",
    "ax.set_title(my_title)\n",
    "\n",
    "# turn off the latitude/longitude axes\n",
    "ax.set_axis_off();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choropleth multiple years\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "For something more interesting let's visualize ex-offender rates for 10 years.\n",
    "\n",
    "This time we'll pull zipcodes within Cook County, IL to limit the amount of data in our animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# query for list of zipcodes:\n",
    "query = \"\"\"SELECT DISTINCT z.geoid10 as zipcode \n",
    "FROM tl_2016_us_zcta510 z JOIN tl_2016_us_county c \n",
    "ON ST_Within(z.geom, c.geom)\n",
    "WHERE c.geoid = '17031'; \"\"\"\n",
    "\n",
    "# get the zipcodes as a pandas dataframe\n",
    "zipcodes = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how many zipcodes is this?\n",
    "len(zipcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now convert the zipcode dataframe into a string \n",
    "# with single quotes and commas separating each\n",
    "# to select out just those zipcodes in the following SQL query\n",
    "zipcodes = ','.join([\"'\"+z+\"'\" for z in zipcodes.zipcode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# almost our same query from above Choropleth of rate, with two differences\n",
    "# 1. get year from the exit_date and limit to our most recent 10 years\n",
    "# 2. we add year as a GROUP BY in the subquery\n",
    "qry = \"\"\"SELECT z.gid, z.geoid10, zip5, year::integer, \n",
    "    tot_exoff/pop_est_2010::numeric exoffrate, geom\n",
    "FROM tl_2016_us_zcta510 AS z \n",
    "LEFT JOIN (SELECT zip5, count(docnbr) AS tot_exoff,\n",
    "            extract(year from exit_date) as year\n",
    "       FROM ildoc.ildoc_exit WHERE extract(year from exit_date) > 2005 \n",
    "       GROUP BY zip5, year) AS e \n",
    "ON z.geoid10 = e.zip5 \n",
    "WHERE z.geoid10 IN ({});\"\"\".format(zipcodes)\n",
    "\n",
    "# get new data\n",
    "ex_off_rate_10yr = gpd.read_postgis(qry, engine, geom_col='geom', index_col='gid')\n",
    "\n",
    "# view info of returned GeoDataFrame\n",
    "ex_off_rate_10yr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# view summary stats of full dataset\n",
    "ex_off_rate_10yr.exoffrate.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's make a new column of quantiles to make comparing our years easier\n",
    "ex_off_rate_10yr['quant5'] = ps.Quantiles(ex_off_rate_10yr.exoffrate,k=5).yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for each year, how many records are in each of our quantile buckets?\n",
    "grouped_counts = ex_off_rate_10yr.groupby(['year', 'quant5'])['zip5'].count().reset_index()\n",
    "\n",
    "# what does this dataframe look like?\n",
    "grouped_counts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's see it as a 10x5 table\n",
    "grouped_counts.pivot_table(index='year', columns='quant5', values='zip5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create 2d list of years\n",
    "years = [range(2006, 2011, 1), range(2011, 2016, 1)]\n",
    "years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: following cell takes some time to run so it would be good if people attempt to coordinate in their groups to not all run at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now we'll make a choropleth for each year all on the same figure\n",
    "# using mulitple subplots\n",
    "f, axes = plt.subplots(2, 5, sharex=True, sharey=True, figsize=(10,6))\n",
    "\n",
    "# do two for loops to plot each year in position\n",
    "for row in xrange(2):\n",
    "    for col in xrange(5):\n",
    "        temp_gdf = ex_off_rate_10yr[ex_off_rate_10yr.year==years[row][col]]\n",
    "#         print('temp gdf %s %s created' % (row, col))\n",
    "        temp_gdf.plot('quant5', categorical=True, \\\n",
    "            cmap='viridis_r', linewidth=0.1, ax=axes[row,col], \\\n",
    "            edgecolor='grey')\n",
    "        # also let's turn off the axis lables as they're not so useful here\n",
    "        axes[row,col].set_yticklabels([])\n",
    "        axes[row,col].set_xticklabels([])\n",
    "        # and set the title column as the year\n",
    "        axes[row,col].set_title(str(years[row][col]))\n",
    "# add a title for the whole figure\n",
    "f.suptitle('Rate of ex-offenders by Cook County zipcode by year');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Addendum\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "### Addendum contents\n",
    "1. [Spatial data types](#Spatial-data-types)\n",
    "2. [Datasets](#Datasets) in this notebook\n",
    "2. Projections and [Coordinate Reference Systems (CRS)](#Coordinate-Reference-Systems)\n",
    "2. [Spatial info of tables in PostGIS](#Spatial-info-of-tables-in-PostGIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial data types\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "- Back to [Addendum contents](#Addendum-contents)\n",
    "\n",
    "There are two generic spatial data types:\n",
    "1. **Vector** - discrete data (usually), represented by points, lines, and polygons\n",
    "2. **Raster** - continuous data (usually), generally represented as square pixels where each pixel (or \"grid cell\") has some value. Examples of raster data - link to \"big data\"\n",
    "  * Imagery data (satellite, Google SteetView, traffic cameras, Placemeter)\n",
    "  * Surface data (collected at monitoring stations then interpolated to a 'surface' - eg Array of Things, weather data)\n",
    "  \n",
    "However, raster data is commonly used in few social science contexts, so the below image (courtesy of [Data Science for Social Good](https://github.com/geebioso/postgis-workshop/blob/master/tutorial.org)) is probably sufficiet discussion about rasters for now:\n",
    "![raster](../../data/sample_data/raster_example.png)\n",
    "\n",
    "> Notice the pesky _\"usually\"_ next to both vector and raster datatypes? Technically any data **_could_** be represented as either vector or raster, but it would be computationally inefficient to create a raster layer of rivers or roads across Illinois because \n",
    "1. All the non-road and non-river locations would still have some value and \n",
    "2. You would have to pick a cell size which may not well represent the actual course of a given river (as opposed to a vector - line - that follows a path and could have some value for width of the river or road)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "- Back to [Addendum contents](#Addendum-contents)\n",
    "\n",
    "Datasets used in this Notebook\n",
    "1. Illinois State Prisons - point dataset of state prisons (source -> https://www.google.com/maps/d/u/0/viewer?mid=12vPv_cWo8H-exJs_zD5E4HCnyEA&hl=en_US&ll=39.65011658688028%2C-89.16519449999998&z=7) \n",
    "2. US Counties - polygons of counties from US Census's TIGER\\Lines product (source-> https://www.census.gov/geo/maps-data/data/tiger-line.html)\n",
    "3. US zip codes - polygons of zip codes from US Census's TIGER\\Lines\n",
    "4. IL DOC exits - ex-offender release information (including expected zip code for some), [link to ADRF Explorer page](https://deepdish.adrf.info/detail/adrf-000002)\n",
    "\n",
    "Considerations for spatial data\n",
    "+ Data collection - at what spatial scale were data collected?\n",
    "+ Data aggregation issue: aggregating to different spatial units could give different results due to something known as the \"modifiable areal unit problem\" (MAUP). MAUP is a statistical bias from summarizing point data to areas as the value of a given area depends on where the boundary is drawn (description paraphrased from Wikipedia, which has more info and links to related issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list files (with details) in data_dir_1:\n",
    "!ls -lh {data_dir_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common type of vector data is called a \"shapefile\". Shapefiles are actually a collection of files (eg the `IL_prisons.*` files above) and there are:\n",
    "1. Required files\n",
    "  + .dbf - contains attributes (ie variables that describe the object, like number of households in a zip code)\n",
    "  + .shp - the geographic information of the object\n",
    "  + .shx - an index to facilitate searching within data\n",
    "2. Additional / optional files\n",
    "  + .prj - the projetion or coordinate reference sysstem (CRS); technically optional but **highly** recommended as it tells you how the data relates to real-world locations\n",
    "  + .sbn / .sbx - other indexes used by specific software\n",
    "  + .shp.xml - metadata information\n",
    "  \n",
    "> Above description pulled mostly from the Wikipedia article on [shapefiles](https://en.wikipedia.org/wiki/Shapefile) which also has much more information.\n",
    "\n",
    "Other common vector formats:\n",
    "1. GeoJSON - Geospatial standards in the JavaScript Object Notation (JSON) text format\n",
    "2. KML / KMZ - Keyhole Markup Language popularized by Google\n",
    "3. GDB - Esri's Geodatabase, a proprietary data format used in ArcGIS software products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate Reference Systems\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "- Back to [Addendum contents](#Addendum-contents)\n",
    "\n",
    "Coordinate Reference Systems (aka projections) are basically math that (1) describes how information in a given dataset relates to the rest of the world and (2) usually creates a 'flat' surface on which data can be analyzed using more common algorithms (eg Euclidean geometry). \n",
    "\n",
    ">Why do we care?\n",
    "1. Distance / area measurements\n",
    "2. Spatial join - won't work with different CRS\n",
    "\n",
    "\n",
    "As an example of point 2, consider the distance between two points: Euclidean distance (aka pythagorean theorem) provides an easy way to calculate distance so long as we know the difference in **_x_** (longitude) and **_y_** (latitude) between two points:\n",
    "$$Distance   = \\sqrt(({x}_1-{x}_2)^2 + ({y}_1-{y}_2)^2)$$\n",
    "\n",
    "This works fine on **_correctly projected_** data, but **_does not work_** on unprojected data. For one thing the result would be in degrees and degrees are a different distance apart (in terms of meters or miles) at different points on the Earth's surface.\n",
    "\n",
    "All this is to say: if you do a calculation with geographic data and the numbers don't make sense, check the projection. Let's do an example with the IL county areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create DB connection (connection string is \"<database-type>://<username>@<host-url>:<port>/<database-name>\")\n",
    "engine = create_engine(\"postgresql://10.10.2.10:5432/appliedda\")\n",
    "\n",
    "# create SQL query - limit to just IL by using the state FIPS code of 17\n",
    "sql = \"SELECT * FROM tl_2016_us_county WHERE statefp = '17';\"\n",
    "\n",
    "# get data from the database\n",
    "il_counties = gpd.read_postgis(sql, engine, geom_col='geom', index_col='gid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print out the CRS of IL counties:\n",
    "print(il_counties.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so first, it needs to be set (I'd guess GeoPandas will appropriately set from a database in the future). If we look it up in the database we'll see that it's NAD83 (North American Datum 1983), which has the [EPSG](www.epsg.org) (European Petroleum Survey Group) code of 4269."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the counties crs to 4269\n",
    "il_counties.crs = {'init': u'epsg:4269'}\n",
    "\n",
    "# print it out\n",
    "print(il_counties.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's check out the area calculated using Pandas with NAD83\n",
    "il_counties['area_nad83'] = il_counties.geom.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# view the first 5 records' aland and calculated area with NAD83:\n",
    "il_counties.loc[:,('aland', 'area_nad83')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly not the same. We can look for other projections at a number of websites, but a good one is [epsg.io](www.epsg.io). let's use the US National Atlas Equal Area projection (epsg=2163), which is a meters based equal area projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform aka re-project the data (use the \"inplace=True\" flag to perform the operation on this Geodataframe)\n",
    "il_counties.to_crs(epsg=2163, inplace=True)\n",
    "\n",
    "# print out the CRS to see it worked\n",
    "print(il_counties.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# and let's calculate the area with the new CRS\n",
    "il_counties['area_2163'] = il_counties.geom.area\n",
    "\n",
    "# and again check the head() of the data, with all 3 area columns:\n",
    "il_counties.loc[:,('aland', 'area_nad83', 'area_2163')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's check if those small differences are just because we're only looking at land area, not full county area:\n",
    "il_counties['total_area'] = il_counties.aland + il_counties.awater\n",
    "\n",
    "# and recheck areas against total:\n",
    "il_counties.loc[:,('total_area', 'area_nad83', 'area_2163')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are still some differences between our newly calculated area ('area_2163') and the total area that came in the data ('aland' + 'awater'), however we can see it's much closer than the nad83 version. These small differences most likely mean that the area from Census was calculated using a different Coordinate Reference System."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Spatial info of tables in PostGIS\n",
    "\n",
    "- Back to the [Table of Contents](#Table-of-Contents)\n",
    "- Back to the [Addendum contents](#Addendum-contents)\n",
    "\n",
    "In PostGIS the `geometry_columns` view maintains information about what geometry data exists in the tables in the database, so there's a simple way to see _all_ the tables that have spatial data like so:\n",
    "\n",
    "    SELECT f_table_name\n",
    "    FROM geometry_columns\n",
    "    GROUP BY f_table_name\n",
    "    ORDER BY f_table_name;\n",
    "    \n",
    "And there is also a way to see what spatial columns exist in a given table, along with their SRID and datatype:\n",
    "\n",
    "    SELECT f_geometry_column, srid, type\n",
    "    FROM geometry_columns WHERE\n",
    "    f_table_name = 'tl_2016_us_zcta510';\n",
    "\n",
    "> Reminder: the SRID is the Spatial Reference Identifier, a unique code used in the `spatial_ref_sys` table as described in the Coordinate Reference System section above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
